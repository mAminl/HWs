{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeEI9noRhNzAjXaUR5OveK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mAminl/HWs/blob/master/HW2Q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Name: MohammadAmin Latifi\n",
        "\n",
        "Student Number: 401300966\n",
        "\n",
        "HW 2, Question 1: \n",
        "\n",
        "Part a)"
      ],
      "metadata": {
        "id": "69Qq3kEO5ZpV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import string\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from matplotlib import rc\n",
        "from google.colab import drive\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "Data = pd.read_csv('/Q3_train.csv', sep=',')\n",
        "train = torch.tensor( data = Data.values, requires_grad = False ).int()\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3,3\n",
        "unique_labels = []\n",
        "k = 1\n",
        "while len(unique_labels) < 9 :\n",
        "    length = len(unique_labels)\n",
        "    sample_idx = torch.randint(len(train), size=(1,)).item()\n",
        "    a = train[sample_idx ,1:].numpy()\n",
        "    img = Image.fromarray(a.reshape(28,28))\n",
        "    label = chr(ord('@')+int(train[sample_idx,0].numpy())+1)\n",
        "    unique_labels.append(label) if label not in unique_labels else unique_labels\n",
        "    if (len(unique_labels) > length):\n",
        "      figure.add_subplot(rows, cols, k)\n",
        "      plt.title(label)\n",
        "      plt.axis(\"off\")\n",
        "      plt.imshow(img, cmap=\"gray\")\n",
        "      k+=1\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "fzenbVOb5c9R",
        "outputId": "bd673c79-a44c-45f8-81ec-5cbd3cfad933"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part b):"
      ],
      "metadata": {
        "id": "zID6Umgiro4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_targets = torch.nn.functional.one_hot(train[:,0].to(torch.int64))\n",
        "train = torch.cat((train[:,1:],train_targets),1)\n",
        "\n",
        "\n",
        "Valid,Train = torch.split(train,[int(len(train)/5),len(train)-int(len(train)/5)])\n",
        "print(f'Validation data shape is: {Valid.shape}')\n",
        "print(f'Train data shape is: {Train.shape}')\n",
        "train_dataloader = DataLoader(Train, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(Valid, batch_size=16, shuffle=True)\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 25)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device).float()\n",
        "print(model)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, X in enumerate(dataloader):\n",
        "        X = X.to(device).float()\n",
        "        x = torch.nn.functional.normalize(X[:,:-25], p=2.0, dim=1, eps=1e-12, out=None)\n",
        "        y = X[:,-25:]\n",
        "        # Compute prediction error\n",
        "        pred = model(x)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X in dataloader:\n",
        "            X = X.to(device).float()\n",
        "            x = torch.nn.functional.normalize(X[:,:-25], p=2.0, dim=1, eps=1e-12, out=None)\n",
        "            y = X[:,-25:]\n",
        "            pred = model(x)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y.argmax(1)).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "epochs = 40\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(valid_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej8b3xvwrq6n",
        "outputId": "3184138d-46f0-4837-90f9-a9347106ca69"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data shape is: torch.Size([4009, 809])\n",
            "Train data shape is: torch.Size([16040, 809])\n",
            "Using cpu device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=128, out_features=25, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.228287  [    0/16040]\n",
            "loss: 3.221693  [ 1600/16040]\n",
            "loss: 3.223173  [ 3200/16040]\n",
            "loss: 3.191238  [ 4800/16040]\n",
            "loss: 3.157642  [ 6400/16040]\n",
            "loss: 3.202770  [ 8000/16040]\n",
            "loss: 3.200505  [ 9600/16040]\n",
            "loss: 3.192140  [11200/16040]\n",
            "loss: 3.215097  [12800/16040]\n",
            "loss: 3.138243  [14400/16040]\n",
            "loss: 3.202154  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 4.8%, Avg loss: 3.181603 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 3.191786  [    0/16040]\n",
            "loss: 3.153165  [ 1600/16040]\n",
            "loss: 3.196529  [ 3200/16040]\n",
            "loss: 3.215324  [ 4800/16040]\n",
            "loss: 3.201559  [ 6400/16040]\n",
            "loss: 3.151521  [ 8000/16040]\n",
            "loss: 3.179538  [ 9600/16040]\n",
            "loss: 3.175287  [11200/16040]\n",
            "loss: 3.167706  [12800/16040]\n",
            "loss: 3.138186  [14400/16040]\n",
            "loss: 3.184776  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 5.0%, Avg loss: 3.179834 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 3.184878  [    0/16040]\n",
            "loss: 3.164369  [ 1600/16040]\n",
            "loss: 3.202761  [ 3200/16040]\n",
            "loss: 3.174776  [ 4800/16040]\n",
            "loss: 3.188469  [ 6400/16040]\n",
            "loss: 3.211207  [ 8000/16040]\n",
            "loss: 3.165477  [ 9600/16040]\n",
            "loss: 3.156436  [11200/16040]\n",
            "loss: 3.193763  [12800/16040]\n",
            "loss: 3.178089  [14400/16040]\n",
            "loss: 3.180793  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 4.3%, Avg loss: 3.175565 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 3.232636  [    0/16040]\n",
            "loss: 3.149528  [ 1600/16040]\n",
            "loss: 3.168893  [ 3200/16040]\n",
            "loss: 3.233433  [ 4800/16040]\n",
            "loss: 3.134773  [ 6400/16040]\n",
            "loss: 3.166349  [ 8000/16040]\n",
            "loss: 3.212551  [ 9600/16040]\n",
            "loss: 3.156655  [11200/16040]\n",
            "loss: 3.172121  [12800/16040]\n",
            "loss: 3.091332  [14400/16040]\n",
            "loss: 3.217620  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 8.7%, Avg loss: 3.101540 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 3.095764  [    0/16040]\n",
            "loss: 3.178411  [ 1600/16040]\n",
            "loss: 2.869151  [ 3200/16040]\n",
            "loss: 2.781104  [ 4800/16040]\n",
            "loss: 2.822973  [ 6400/16040]\n",
            "loss: 2.981426  [ 8000/16040]\n",
            "loss: 2.963688  [ 9600/16040]\n",
            "loss: 2.787462  [11200/16040]\n",
            "loss: 3.019865  [12800/16040]\n",
            "loss: 2.571011  [14400/16040]\n",
            "loss: 2.779237  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 13.4%, Avg loss: 2.763713 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.608978  [    0/16040]\n",
            "loss: 2.948950  [ 1600/16040]\n",
            "loss: 2.935451  [ 3200/16040]\n",
            "loss: 2.911445  [ 4800/16040]\n",
            "loss: 3.012155  [ 6400/16040]\n",
            "loss: 2.734035  [ 8000/16040]\n",
            "loss: 2.657835  [ 9600/16040]\n",
            "loss: 3.125451  [11200/16040]\n",
            "loss: 2.573954  [12800/16040]\n",
            "loss: 2.466861  [14400/16040]\n",
            "loss: 2.461472  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 7.9%, Avg loss: 2.951368 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.832637  [    0/16040]\n",
            "loss: 2.654750  [ 1600/16040]\n",
            "loss: 2.644232  [ 3200/16040]\n",
            "loss: 2.418405  [ 4800/16040]\n",
            "loss: 2.079246  [ 6400/16040]\n",
            "loss: 2.566152  [ 8000/16040]\n",
            "loss: 2.763103  [ 9600/16040]\n",
            "loss: 2.342515  [11200/16040]\n",
            "loss: 2.193532  [12800/16040]\n",
            "loss: 2.211564  [14400/16040]\n",
            "loss: 2.177437  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 21.5%, Avg loss: 2.258234 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.984109  [    0/16040]\n",
            "loss: 2.338934  [ 1600/16040]\n",
            "loss: 2.270130  [ 3200/16040]\n",
            "loss: 2.066819  [ 4800/16040]\n",
            "loss: 2.137827  [ 6400/16040]\n",
            "loss: 3.737461  [ 8000/16040]\n",
            "loss: 2.507671  [ 9600/16040]\n",
            "loss: 2.001780  [11200/16040]\n",
            "loss: 1.731853  [12800/16040]\n",
            "loss: 1.823397  [14400/16040]\n",
            "loss: 2.277652  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 30.0%, Avg loss: 1.995285 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.041108  [    0/16040]\n",
            "loss: 2.177974  [ 1600/16040]\n",
            "loss: 2.256252  [ 3200/16040]\n",
            "loss: 2.096883  [ 4800/16040]\n",
            "loss: 2.610632  [ 6400/16040]\n",
            "loss: 1.988131  [ 8000/16040]\n",
            "loss: 2.052919  [ 9600/16040]\n",
            "loss: 1.904585  [11200/16040]\n",
            "loss: 2.036269  [12800/16040]\n",
            "loss: 2.459485  [14400/16040]\n",
            "loss: 2.785090  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 32.7%, Avg loss: 1.944609 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.101699  [    0/16040]\n",
            "loss: 2.116134  [ 1600/16040]\n",
            "loss: 1.863823  [ 3200/16040]\n",
            "loss: 1.807324  [ 4800/16040]\n",
            "loss: 1.930228  [ 6400/16040]\n",
            "loss: 2.011827  [ 8000/16040]\n",
            "loss: 1.798110  [ 9600/16040]\n",
            "loss: 2.271344  [11200/16040]\n",
            "loss: 2.345512  [12800/16040]\n",
            "loss: 2.620721  [14400/16040]\n",
            "loss: 1.873635  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 37.6%, Avg loss: 1.729846 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.911855  [    0/16040]\n",
            "loss: 1.634792  [ 1600/16040]\n",
            "loss: 1.536399  [ 3200/16040]\n",
            "loss: 1.797649  [ 4800/16040]\n",
            "loss: 1.534023  [ 6400/16040]\n",
            "loss: 1.814039  [ 8000/16040]\n",
            "loss: 1.196881  [ 9600/16040]\n",
            "loss: 1.659645  [11200/16040]\n",
            "loss: 1.770571  [12800/16040]\n",
            "loss: 1.785745  [14400/16040]\n",
            "loss: 0.852354  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 38.1%, Avg loss: 1.718965 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.195385  [    0/16040]\n",
            "loss: 1.969653  [ 1600/16040]\n",
            "loss: 1.786967  [ 3200/16040]\n",
            "loss: 1.835886  [ 4800/16040]\n",
            "loss: 1.655562  [ 6400/16040]\n",
            "loss: 1.924554  [ 8000/16040]\n",
            "loss: 1.872083  [ 9600/16040]\n",
            "loss: 1.071741  [11200/16040]\n",
            "loss: 2.202631  [12800/16040]\n",
            "loss: 1.190035  [14400/16040]\n",
            "loss: 1.870745  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 42.1%, Avg loss: 1.535076 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.622903  [    0/16040]\n",
            "loss: 1.281583  [ 1600/16040]\n",
            "loss: 1.101390  [ 3200/16040]\n",
            "loss: 1.263571  [ 4800/16040]\n",
            "loss: 1.783331  [ 6400/16040]\n",
            "loss: 1.213506  [ 8000/16040]\n",
            "loss: 2.319722  [ 9600/16040]\n",
            "loss: 1.079506  [11200/16040]\n",
            "loss: 1.036014  [12800/16040]\n",
            "loss: 1.596355  [14400/16040]\n",
            "loss: 1.317832  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 48.0%, Avg loss: 1.460112 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.642043  [    0/16040]\n",
            "loss: 1.231571  [ 1600/16040]\n",
            "loss: 1.105521  [ 3200/16040]\n",
            "loss: 1.217646  [ 4800/16040]\n",
            "loss: 1.450923  [ 6400/16040]\n",
            "loss: 1.552087  [ 8000/16040]\n",
            "loss: 1.893355  [ 9600/16040]\n",
            "loss: 1.494609  [11200/16040]\n",
            "loss: 1.264104  [12800/16040]\n",
            "loss: 2.083972  [14400/16040]\n",
            "loss: 1.136326  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 39.2%, Avg loss: 1.814845 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.218676  [    0/16040]\n",
            "loss: 1.025015  [ 1600/16040]\n",
            "loss: 1.328610  [ 3200/16040]\n",
            "loss: 1.290625  [ 4800/16040]\n",
            "loss: 1.316133  [ 6400/16040]\n",
            "loss: 1.521855  [ 8000/16040]\n",
            "loss: 1.262645  [ 9600/16040]\n",
            "loss: 0.986070  [11200/16040]\n",
            "loss: 1.201452  [12800/16040]\n",
            "loss: 1.397969  [14400/16040]\n",
            "loss: 1.351747  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 27.2%, Avg loss: 2.783121 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 3.412332  [    0/16040]\n",
            "loss: 1.348159  [ 1600/16040]\n",
            "loss: 1.416760  [ 3200/16040]\n",
            "loss: 1.138536  [ 4800/16040]\n",
            "loss: 1.316250  [ 6400/16040]\n",
            "loss: 1.260734  [ 8000/16040]\n",
            "loss: 1.097679  [ 9600/16040]\n",
            "loss: 0.953000  [11200/16040]\n",
            "loss: 1.289898  [12800/16040]\n",
            "loss: 2.030456  [14400/16040]\n",
            "loss: 0.731792  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 60.9%, Avg loss: 1.071919 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.237626  [    0/16040]\n",
            "loss: 0.910971  [ 1600/16040]\n",
            "loss: 1.135908  [ 3200/16040]\n",
            "loss: 1.271171  [ 4800/16040]\n",
            "loss: 1.350591  [ 6400/16040]\n",
            "loss: 1.206297  [ 8000/16040]\n",
            "loss: 1.530911  [ 9600/16040]\n",
            "loss: 1.365234  [11200/16040]\n",
            "loss: 0.894526  [12800/16040]\n",
            "loss: 1.507446  [14400/16040]\n",
            "loss: 0.919884  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 60.7%, Avg loss: 1.047933 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.138696  [    0/16040]\n",
            "loss: 1.124499  [ 1600/16040]\n",
            "loss: 1.138857  [ 3200/16040]\n",
            "loss: 1.238905  [ 4800/16040]\n",
            "loss: 0.633034  [ 6400/16040]\n",
            "loss: 0.474230  [ 8000/16040]\n",
            "loss: 0.630126  [ 9600/16040]\n",
            "loss: 0.608799  [11200/16040]\n",
            "loss: 0.739984  [12800/16040]\n",
            "loss: 0.785112  [14400/16040]\n",
            "loss: 0.806350  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 71.9%, Avg loss: 0.800246 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.702232  [    0/16040]\n",
            "loss: 1.036288  [ 1600/16040]\n",
            "loss: 0.721847  [ 3200/16040]\n",
            "loss: 2.239956  [ 4800/16040]\n",
            "loss: 0.944708  [ 6400/16040]\n",
            "loss: 0.621094  [ 8000/16040]\n",
            "loss: 0.554552  [ 9600/16040]\n",
            "loss: 0.545495  [11200/16040]\n",
            "loss: 0.556527  [12800/16040]\n",
            "loss: 0.930400  [14400/16040]\n",
            "loss: 0.577515  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 77.2%, Avg loss: 0.644538 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.305397  [    0/16040]\n",
            "loss: 0.689602  [ 1600/16040]\n",
            "loss: 0.632462  [ 3200/16040]\n",
            "loss: 0.547892  [ 4800/16040]\n",
            "loss: 0.410973  [ 6400/16040]\n",
            "loss: 0.693661  [ 8000/16040]\n",
            "loss: 0.407418  [ 9600/16040]\n",
            "loss: 1.659266  [11200/16040]\n",
            "loss: 0.611835  [12800/16040]\n",
            "loss: 0.783199  [14400/16040]\n",
            "loss: 0.488907  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.782865 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.747676  [    0/16040]\n",
            "loss: 0.997506  [ 1600/16040]\n",
            "loss: 0.357735  [ 3200/16040]\n",
            "loss: 0.736555  [ 4800/16040]\n",
            "loss: 0.592782  [ 6400/16040]\n",
            "loss: 0.580236  [ 8000/16040]\n",
            "loss: 0.582025  [ 9600/16040]\n",
            "loss: 0.777140  [11200/16040]\n",
            "loss: 1.021073  [12800/16040]\n",
            "loss: 0.364933  [14400/16040]\n",
            "loss: 0.476320  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 72.5%, Avg loss: 0.773731 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.454759  [    0/16040]\n",
            "loss: 0.786066  [ 1600/16040]\n",
            "loss: 0.346228  [ 3200/16040]\n",
            "loss: 0.506275  [ 4800/16040]\n",
            "loss: 0.385613  [ 6400/16040]\n",
            "loss: 0.255646  [ 8000/16040]\n",
            "loss: 0.422470  [ 9600/16040]\n",
            "loss: 0.328027  [11200/16040]\n",
            "loss: 0.989749  [12800/16040]\n",
            "loss: 0.851593  [14400/16040]\n",
            "loss: 0.234128  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 66.4%, Avg loss: 0.903161 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1.401747  [    0/16040]\n",
            "loss: 0.084028  [ 1600/16040]\n",
            "loss: 0.343786  [ 3200/16040]\n",
            "loss: 0.849076  [ 4800/16040]\n",
            "loss: 1.411832  [ 6400/16040]\n",
            "loss: 0.437756  [ 8000/16040]\n",
            "loss: 0.660272  [ 9600/16040]\n",
            "loss: 1.909430  [11200/16040]\n",
            "loss: 0.286355  [12800/16040]\n",
            "loss: 0.392392  [14400/16040]\n",
            "loss: 0.394898  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 52.1%, Avg loss: 1.717417 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 1.984157  [    0/16040]\n",
            "loss: 0.382245  [ 1600/16040]\n",
            "loss: 0.357986  [ 3200/16040]\n",
            "loss: 0.465135  [ 4800/16040]\n",
            "loss: 0.216319  [ 6400/16040]\n",
            "loss: 0.361327  [ 8000/16040]\n",
            "loss: 0.406765  [ 9600/16040]\n",
            "loss: 0.447595  [11200/16040]\n",
            "loss: 1.125882  [12800/16040]\n",
            "loss: 0.231055  [14400/16040]\n",
            "loss: 0.151920  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 89.7%, Avg loss: 0.304240 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.102376  [    0/16040]\n",
            "loss: 0.341746  [ 1600/16040]\n",
            "loss: 0.462774  [ 3200/16040]\n",
            "loss: 2.053509  [ 4800/16040]\n",
            "loss: 0.511868  [ 6400/16040]\n",
            "loss: 0.192944  [ 8000/16040]\n",
            "loss: 0.585415  [ 9600/16040]\n",
            "loss: 0.505563  [11200/16040]\n",
            "loss: 0.221154  [12800/16040]\n",
            "loss: 0.143998  [14400/16040]\n",
            "loss: 1.093446  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 45.1%, Avg loss: 2.518697 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 2.671227  [    0/16040]\n",
            "loss: 0.258728  [ 1600/16040]\n",
            "loss: 0.205406  [ 3200/16040]\n",
            "loss: 0.128463  [ 4800/16040]\n",
            "loss: 0.088547  [ 6400/16040]\n",
            "loss: 0.287283  [ 8000/16040]\n",
            "loss: 0.176622  [ 9600/16040]\n",
            "loss: 0.348819  [11200/16040]\n",
            "loss: 0.449713  [12800/16040]\n",
            "loss: 0.147665  [14400/16040]\n",
            "loss: 0.180717  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 88.8%, Avg loss: 0.320001 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.295363  [    0/16040]\n",
            "loss: 0.112608  [ 1600/16040]\n",
            "loss: 0.210605  [ 3200/16040]\n",
            "loss: 0.282487  [ 4800/16040]\n",
            "loss: 0.112681  [ 6400/16040]\n",
            "loss: 0.183007  [ 8000/16040]\n",
            "loss: 0.179200  [ 9600/16040]\n",
            "loss: 0.474018  [11200/16040]\n",
            "loss: 0.134880  [12800/16040]\n",
            "loss: 0.132770  [14400/16040]\n",
            "loss: 0.076628  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 80.2%, Avg loss: 0.612825 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1.126861  [    0/16040]\n",
            "loss: 0.148843  [ 1600/16040]\n",
            "loss: 0.242443  [ 3200/16040]\n",
            "loss: 0.162549  [ 4800/16040]\n",
            "loss: 0.126285  [ 6400/16040]\n",
            "loss: 0.186558  [ 8000/16040]\n",
            "loss: 0.260889  [ 9600/16040]\n",
            "loss: 0.267729  [11200/16040]\n",
            "loss: 0.082490  [12800/16040]\n",
            "loss: 0.216288  [14400/16040]\n",
            "loss: 0.469344  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 83.0%, Avg loss: 0.483934 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.085612  [    0/16040]\n",
            "loss: 0.180779  [ 1600/16040]\n",
            "loss: 0.296556  [ 3200/16040]\n",
            "loss: 0.081573  [ 4800/16040]\n",
            "loss: 0.037577  [ 6400/16040]\n",
            "loss: 0.919275  [ 8000/16040]\n",
            "loss: 0.051366  [ 9600/16040]\n",
            "loss: 0.140684  [11200/16040]\n",
            "loss: 0.825826  [12800/16040]\n",
            "loss: 0.220957  [14400/16040]\n",
            "loss: 0.119663  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.9%, Avg loss: 0.180494 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.136599  [    0/16040]\n",
            "loss: 3.489123  [ 1600/16040]\n",
            "loss: 0.364523  [ 3200/16040]\n",
            "loss: 0.121928  [ 4800/16040]\n",
            "loss: 0.214378  [ 6400/16040]\n",
            "loss: 0.197611  [ 8000/16040]\n",
            "loss: 0.176258  [ 9600/16040]\n",
            "loss: 0.104778  [11200/16040]\n",
            "loss: 0.148026  [12800/16040]\n",
            "loss: 0.131798  [14400/16040]\n",
            "loss: 0.210973  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.077774 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.017108  [    0/16040]\n",
            "loss: 1.009164  [ 1600/16040]\n",
            "loss: 0.099899  [ 3200/16040]\n",
            "loss: 0.102060  [ 4800/16040]\n",
            "loss: 0.149107  [ 6400/16040]\n",
            "loss: 0.055043  [ 8000/16040]\n",
            "loss: 0.069265  [ 9600/16040]\n",
            "loss: 0.147394  [11200/16040]\n",
            "loss: 0.066110  [12800/16040]\n",
            "loss: 0.031560  [14400/16040]\n",
            "loss: 0.181015  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.1%, Avg loss: 0.621861 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.695251  [    0/16040]\n",
            "loss: 0.048444  [ 1600/16040]\n",
            "loss: 0.042787  [ 3200/16040]\n",
            "loss: 0.733253  [ 4800/16040]\n",
            "loss: 0.095923  [ 6400/16040]\n",
            "loss: 0.190950  [ 8000/16040]\n",
            "loss: 0.098974  [ 9600/16040]\n",
            "loss: 0.111912  [11200/16040]\n",
            "loss: 0.085647  [12800/16040]\n",
            "loss: 0.504822  [14400/16040]\n",
            "loss: 0.137033  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.063162 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.013798  [    0/16040]\n",
            "loss: 0.105315  [ 1600/16040]\n",
            "loss: 0.036240  [ 3200/16040]\n",
            "loss: 0.083349  [ 4800/16040]\n",
            "loss: 0.053188  [ 6400/16040]\n",
            "loss: 0.009562  [ 8000/16040]\n",
            "loss: 0.011732  [ 9600/16040]\n",
            "loss: 0.045182  [11200/16040]\n",
            "loss: 0.108931  [12800/16040]\n",
            "loss: 0.007246  [14400/16040]\n",
            "loss: 0.042689  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.5%, Avg loss: 0.088647 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.036437  [    0/16040]\n",
            "loss: 0.064956  [ 1600/16040]\n",
            "loss: 0.014052  [ 3200/16040]\n",
            "loss: 0.045903  [ 4800/16040]\n",
            "loss: 0.488622  [ 6400/16040]\n",
            "loss: 0.161239  [ 8000/16040]\n",
            "loss: 0.018983  [ 9600/16040]\n",
            "loss: 0.029888  [11200/16040]\n",
            "loss: 0.040128  [12800/16040]\n",
            "loss: 0.068925  [14400/16040]\n",
            "loss: 0.027054  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.1%, Avg loss: 0.070452 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.047241  [    0/16040]\n",
            "loss: 0.063691  [ 1600/16040]\n",
            "loss: 0.015671  [ 3200/16040]\n",
            "loss: 0.009494  [ 4800/16040]\n",
            "loss: 0.131127  [ 6400/16040]\n",
            "loss: 0.012774  [ 8000/16040]\n",
            "loss: 0.042573  [ 9600/16040]\n",
            "loss: 0.004574  [11200/16040]\n",
            "loss: 0.116423  [12800/16040]\n",
            "loss: 0.026022  [14400/16040]\n",
            "loss: 0.024280  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.4%, Avg loss: 0.030268 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.070759  [    0/16040]\n",
            "loss: 0.019981  [ 1600/16040]\n",
            "loss: 0.024468  [ 3200/16040]\n",
            "loss: 0.154696  [ 4800/16040]\n",
            "loss: 0.008702  [ 6400/16040]\n",
            "loss: 0.019217  [ 8000/16040]\n",
            "loss: 0.054445  [ 9600/16040]\n",
            "loss: 0.053492  [11200/16040]\n",
            "loss: 0.028476  [12800/16040]\n",
            "loss: 0.001679  [14400/16040]\n",
            "loss: 0.008538  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.014682 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.007173  [    0/16040]\n",
            "loss: 0.005769  [ 1600/16040]\n",
            "loss: 0.005442  [ 3200/16040]\n",
            "loss: 0.016404  [ 4800/16040]\n",
            "loss: 0.011337  [ 6400/16040]\n",
            "loss: 0.009983  [ 8000/16040]\n",
            "loss: 0.006261  [ 9600/16040]\n",
            "loss: 0.001645  [11200/16040]\n",
            "loss: 0.006437  [12800/16040]\n",
            "loss: 0.014194  [14400/16040]\n",
            "loss: 0.052768  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.060820 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.053696  [    0/16040]\n",
            "loss: 0.068629  [ 1600/16040]\n",
            "loss: 0.014426  [ 3200/16040]\n",
            "loss: 0.012077  [ 4800/16040]\n",
            "loss: 0.015075  [ 6400/16040]\n",
            "loss: 0.005462  [ 8000/16040]\n",
            "loss: 0.007373  [ 9600/16040]\n",
            "loss: 0.002261  [11200/16040]\n",
            "loss: 0.006864  [12800/16040]\n",
            "loss: 0.002160  [14400/16040]\n",
            "loss: 2.701134  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 59.7%, Avg loss: 1.328574 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 1.550426  [    0/16040]\n",
            "loss: 0.101597  [ 1600/16040]\n",
            "loss: 0.027528  [ 3200/16040]\n",
            "loss: 0.052853  [ 4800/16040]\n",
            "loss: 0.057028  [ 6400/16040]\n",
            "loss: 0.003182  [ 8000/16040]\n",
            "loss: 0.147156  [ 9600/16040]\n",
            "loss: 0.009855  [11200/16040]\n",
            "loss: 0.023611  [12800/16040]\n",
            "loss: 0.004915  [14400/16040]\n",
            "loss: 0.049212  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.050040 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.060400  [    0/16040]\n",
            "loss: 0.031039  [ 1600/16040]\n",
            "loss: 0.003573  [ 3200/16040]\n",
            "loss: 0.002661  [ 4800/16040]\n",
            "loss: 1.562835  [ 6400/16040]\n",
            "loss: 0.004369  [ 8000/16040]\n",
            "loss: 0.007345  [ 9600/16040]\n",
            "loss: 0.003595  [11200/16040]\n",
            "loss: 0.016825  [12800/16040]\n",
            "loss: 0.007660  [14400/16040]\n",
            "loss: 0.002705  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 100.0%, Avg loss: 0.008193 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(Train, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(Valid, batch_size=16, shuffle=True)\n",
        "model2 = NeuralNetwork().to(device).float()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
        "optimizer.zero_grad()\n",
        "epochs = 30\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model2, loss_fn, optimizer)\n",
        "    test(valid_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FVDTwGjt7Xh",
        "outputId": "61eaf0b8-1824-49df-f402-5554f6c34862"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.238557  [    0/16040]\n",
            "loss: 3.159248  [ 1600/16040]\n",
            "loss: 3.163342  [ 3200/16040]\n",
            "loss: 2.993405  [ 4800/16040]\n",
            "loss: 3.035349  [ 6400/16040]\n",
            "loss: 2.436769  [ 8000/16040]\n",
            "loss: 2.336173  [ 9600/16040]\n",
            "loss: 2.450615  [11200/16040]\n",
            "loss: 2.245486  [12800/16040]\n",
            "loss: 2.158995  [14400/16040]\n",
            "loss: 2.220534  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 27.1%, Avg loss: 2.155647 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.752473  [    0/16040]\n",
            "loss: 1.991144  [ 1600/16040]\n",
            "loss: 2.493144  [ 3200/16040]\n",
            "loss: 2.385125  [ 4800/16040]\n",
            "loss: 1.692652  [ 6400/16040]\n",
            "loss: 1.808601  [ 8000/16040]\n",
            "loss: 2.010842  [ 9600/16040]\n",
            "loss: 2.492184  [11200/16040]\n",
            "loss: 2.047193  [12800/16040]\n",
            "loss: 2.158327  [14400/16040]\n",
            "loss: 2.038559  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 32.1%, Avg loss: 1.900359 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.748298  [    0/16040]\n",
            "loss: 1.900749  [ 1600/16040]\n",
            "loss: 2.221160  [ 3200/16040]\n",
            "loss: 2.138920  [ 4800/16040]\n",
            "loss: 2.059827  [ 6400/16040]\n",
            "loss: 1.800139  [ 8000/16040]\n",
            "loss: 1.899124  [ 9600/16040]\n",
            "loss: 1.710768  [11200/16040]\n",
            "loss: 1.689634  [12800/16040]\n",
            "loss: 1.296161  [14400/16040]\n",
            "loss: 1.274943  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 42.1%, Avg loss: 1.664190 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.413997  [    0/16040]\n",
            "loss: 1.453657  [ 1600/16040]\n",
            "loss: 1.591683  [ 3200/16040]\n",
            "loss: 1.249135  [ 4800/16040]\n",
            "loss: 1.669866  [ 6400/16040]\n",
            "loss: 1.391817  [ 8000/16040]\n",
            "loss: 1.697620  [ 9600/16040]\n",
            "loss: 1.071340  [11200/16040]\n",
            "loss: 1.506393  [12800/16040]\n",
            "loss: 1.146697  [14400/16040]\n",
            "loss: 1.309654  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 49.8%, Avg loss: 1.415098 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.333409  [    0/16040]\n",
            "loss: 1.405366  [ 1600/16040]\n",
            "loss: 1.383814  [ 3200/16040]\n",
            "loss: 1.491765  [ 4800/16040]\n",
            "loss: 1.268460  [ 6400/16040]\n",
            "loss: 1.240061  [ 8000/16040]\n",
            "loss: 1.260007  [ 9600/16040]\n",
            "loss: 0.991595  [11200/16040]\n",
            "loss: 1.619676  [12800/16040]\n",
            "loss: 1.345285  [14400/16040]\n",
            "loss: 0.850606  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 58.1%, Avg loss: 1.158202 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.131495  [    0/16040]\n",
            "loss: 0.986076  [ 1600/16040]\n",
            "loss: 1.029219  [ 3200/16040]\n",
            "loss: 1.025108  [ 4800/16040]\n",
            "loss: 1.480183  [ 6400/16040]\n",
            "loss: 1.115637  [ 8000/16040]\n",
            "loss: 1.062653  [ 9600/16040]\n",
            "loss: 1.196064  [11200/16040]\n",
            "loss: 0.766151  [12800/16040]\n",
            "loss: 1.134485  [14400/16040]\n",
            "loss: 1.242360  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.000794 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.145054  [    0/16040]\n",
            "loss: 0.893157  [ 1600/16040]\n",
            "loss: 1.088288  [ 3200/16040]\n",
            "loss: 1.059433  [ 4800/16040]\n",
            "loss: 0.476711  [ 6400/16040]\n",
            "loss: 0.898549  [ 8000/16040]\n",
            "loss: 1.193107  [ 9600/16040]\n",
            "loss: 1.084816  [11200/16040]\n",
            "loss: 0.971947  [12800/16040]\n",
            "loss: 0.570244  [14400/16040]\n",
            "loss: 0.969191  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 65.3%, Avg loss: 0.932213 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.507057  [    0/16040]\n",
            "loss: 0.853014  [ 1600/16040]\n",
            "loss: 0.733270  [ 3200/16040]\n",
            "loss: 1.047189  [ 4800/16040]\n",
            "loss: 0.766311  [ 6400/16040]\n",
            "loss: 0.862005  [ 8000/16040]\n",
            "loss: 1.408282  [ 9600/16040]\n",
            "loss: 1.292211  [11200/16040]\n",
            "loss: 0.528313  [12800/16040]\n",
            "loss: 0.599195  [14400/16040]\n",
            "loss: 1.009849  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 70.9%, Avg loss: 0.788707 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.605126  [    0/16040]\n",
            "loss: 0.728626  [ 1600/16040]\n",
            "loss: 0.710505  [ 3200/16040]\n",
            "loss: 1.119696  [ 4800/16040]\n",
            "loss: 0.657659  [ 6400/16040]\n",
            "loss: 0.723747  [ 8000/16040]\n",
            "loss: 0.512641  [ 9600/16040]\n",
            "loss: 0.450952  [11200/16040]\n",
            "loss: 0.526031  [12800/16040]\n",
            "loss: 0.885270  [14400/16040]\n",
            "loss: 0.502751  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 70.7%, Avg loss: 0.818248 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.760243  [    0/16040]\n",
            "loss: 0.577840  [ 1600/16040]\n",
            "loss: 0.656567  [ 3200/16040]\n",
            "loss: 0.406651  [ 4800/16040]\n",
            "loss: 0.540480  [ 6400/16040]\n",
            "loss: 0.753862  [ 8000/16040]\n",
            "loss: 0.592324  [ 9600/16040]\n",
            "loss: 0.342021  [11200/16040]\n",
            "loss: 0.554978  [12800/16040]\n",
            "loss: 1.072991  [14400/16040]\n",
            "loss: 0.522940  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 72.9%, Avg loss: 0.713329 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 0.700565  [    0/16040]\n",
            "loss: 0.728710  [ 1600/16040]\n",
            "loss: 0.303267  [ 3200/16040]\n",
            "loss: 0.554768  [ 4800/16040]\n",
            "loss: 0.568645  [ 6400/16040]\n",
            "loss: 0.540993  [ 8000/16040]\n",
            "loss: 0.651323  [ 9600/16040]\n",
            "loss: 0.566708  [11200/16040]\n",
            "loss: 0.540965  [12800/16040]\n",
            "loss: 0.824297  [14400/16040]\n",
            "loss: 0.255368  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 84.7%, Avg loss: 0.428776 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 0.299349  [    0/16040]\n",
            "loss: 0.663306  [ 1600/16040]\n",
            "loss: 0.959185  [ 3200/16040]\n",
            "loss: 0.748689  [ 4800/16040]\n",
            "loss: 0.432061  [ 6400/16040]\n",
            "loss: 0.368283  [ 8000/16040]\n",
            "loss: 0.332133  [ 9600/16040]\n",
            "loss: 0.396996  [11200/16040]\n",
            "loss: 0.429989  [12800/16040]\n",
            "loss: 0.441202  [14400/16040]\n",
            "loss: 0.390250  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 85.7%, Avg loss: 0.399859 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 0.420960  [    0/16040]\n",
            "loss: 0.215414  [ 1600/16040]\n",
            "loss: 0.201762  [ 3200/16040]\n",
            "loss: 0.298871  [ 4800/16040]\n",
            "loss: 0.338277  [ 6400/16040]\n",
            "loss: 0.352590  [ 8000/16040]\n",
            "loss: 0.257521  [ 9600/16040]\n",
            "loss: 0.519115  [11200/16040]\n",
            "loss: 0.257967  [12800/16040]\n",
            "loss: 0.190251  [14400/16040]\n",
            "loss: 0.332126  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 0.306109 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 0.433555  [    0/16040]\n",
            "loss: 0.214668  [ 1600/16040]\n",
            "loss: 0.153833  [ 3200/16040]\n",
            "loss: 0.581247  [ 4800/16040]\n",
            "loss: 0.151209  [ 6400/16040]\n",
            "loss: 0.139574  [ 8000/16040]\n",
            "loss: 0.303418  [ 9600/16040]\n",
            "loss: 0.189882  [11200/16040]\n",
            "loss: 0.141621  [12800/16040]\n",
            "loss: 0.442274  [14400/16040]\n",
            "loss: 0.217577  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 87.0%, Avg loss: 0.370520 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 0.270191  [    0/16040]\n",
            "loss: 0.373215  [ 1600/16040]\n",
            "loss: 0.282274  [ 3200/16040]\n",
            "loss: 0.357580  [ 4800/16040]\n",
            "loss: 0.381300  [ 6400/16040]\n",
            "loss: 0.293024  [ 8000/16040]\n",
            "loss: 0.199827  [ 9600/16040]\n",
            "loss: 0.182419  [11200/16040]\n",
            "loss: 0.308674  [12800/16040]\n",
            "loss: 0.160289  [14400/16040]\n",
            "loss: 0.472799  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.210657 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 0.246329  [    0/16040]\n",
            "loss: 0.295504  [ 1600/16040]\n",
            "loss: 0.238757  [ 3200/16040]\n",
            "loss: 0.306443  [ 4800/16040]\n",
            "loss: 0.373852  [ 6400/16040]\n",
            "loss: 0.478641  [ 8000/16040]\n",
            "loss: 0.145267  [ 9600/16040]\n",
            "loss: 0.048168  [11200/16040]\n",
            "loss: 0.339334  [12800/16040]\n",
            "loss: 0.160008  [14400/16040]\n",
            "loss: 0.155522  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.5%, Avg loss: 0.188131 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.033491  [    0/16040]\n",
            "loss: 0.197741  [ 1600/16040]\n",
            "loss: 0.472971  [ 3200/16040]\n",
            "loss: 0.181507  [ 4800/16040]\n",
            "loss: 0.337495  [ 6400/16040]\n",
            "loss: 0.082531  [ 8000/16040]\n",
            "loss: 0.204979  [ 9600/16040]\n",
            "loss: 0.118760  [11200/16040]\n",
            "loss: 0.041235  [12800/16040]\n",
            "loss: 0.473772  [14400/16040]\n",
            "loss: 0.232429  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.163305 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.018615  [    0/16040]\n",
            "loss: 0.170079  [ 1600/16040]\n",
            "loss: 0.075127  [ 3200/16040]\n",
            "loss: 0.307927  [ 4800/16040]\n",
            "loss: 0.221765  [ 6400/16040]\n",
            "loss: 0.075672  [ 8000/16040]\n",
            "loss: 0.076534  [ 9600/16040]\n",
            "loss: 0.048173  [11200/16040]\n",
            "loss: 0.069787  [12800/16040]\n",
            "loss: 0.271446  [14400/16040]\n",
            "loss: 0.171544  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.6%, Avg loss: 0.144641 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 0.494981  [    0/16040]\n",
            "loss: 0.113244  [ 1600/16040]\n",
            "loss: 0.085713  [ 3200/16040]\n",
            "loss: 0.138388  [ 4800/16040]\n",
            "loss: 0.135088  [ 6400/16040]\n",
            "loss: 0.079967  [ 8000/16040]\n",
            "loss: 0.038318  [ 9600/16040]\n",
            "loss: 0.243943  [11200/16040]\n",
            "loss: 0.011044  [12800/16040]\n",
            "loss: 0.396753  [14400/16040]\n",
            "loss: 0.182381  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 95.3%, Avg loss: 0.140571 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.083160  [    0/16040]\n",
            "loss: 0.112658  [ 1600/16040]\n",
            "loss: 0.070193  [ 3200/16040]\n",
            "loss: 0.388177  [ 4800/16040]\n",
            "loss: 0.206525  [ 6400/16040]\n",
            "loss: 0.123282  [ 8000/16040]\n",
            "loss: 0.080029  [ 9600/16040]\n",
            "loss: 0.065708  [11200/16040]\n",
            "loss: 0.032289  [12800/16040]\n",
            "loss: 0.039945  [14400/16040]\n",
            "loss: 0.014300  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.4%, Avg loss: 0.156469 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 0.360130  [    0/16040]\n",
            "loss: 0.271459  [ 1600/16040]\n",
            "loss: 0.049554  [ 3200/16040]\n",
            "loss: 0.034668  [ 4800/16040]\n",
            "loss: 0.201259  [ 6400/16040]\n",
            "loss: 0.096078  [ 8000/16040]\n",
            "loss: 0.058081  [ 9600/16040]\n",
            "loss: 0.033826  [11200/16040]\n",
            "loss: 0.017873  [12800/16040]\n",
            "loss: 0.172714  [14400/16040]\n",
            "loss: 0.038554  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.5%, Avg loss: 0.039707 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.033863  [    0/16040]\n",
            "loss: 0.042483  [ 1600/16040]\n",
            "loss: 0.036397  [ 3200/16040]\n",
            "loss: 0.105594  [ 4800/16040]\n",
            "loss: 0.128961  [ 6400/16040]\n",
            "loss: 0.189843  [ 8000/16040]\n",
            "loss: 0.007992  [ 9600/16040]\n",
            "loss: 0.006185  [11200/16040]\n",
            "loss: 0.006483  [12800/16040]\n",
            "loss: 0.246219  [14400/16040]\n",
            "loss: 0.120572  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.066539 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.072401  [    0/16040]\n",
            "loss: 0.038123  [ 1600/16040]\n",
            "loss: 0.036440  [ 3200/16040]\n",
            "loss: 0.001145  [ 4800/16040]\n",
            "loss: 0.023921  [ 6400/16040]\n",
            "loss: 0.012397  [ 8000/16040]\n",
            "loss: 0.630884  [ 9600/16040]\n",
            "loss: 0.062806  [11200/16040]\n",
            "loss: 0.031397  [12800/16040]\n",
            "loss: 0.003902  [14400/16040]\n",
            "loss: 0.018607  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.038492 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.047933  [    0/16040]\n",
            "loss: 0.005487  [ 1600/16040]\n",
            "loss: 0.077572  [ 3200/16040]\n",
            "loss: 0.002459  [ 4800/16040]\n",
            "loss: 0.097462  [ 6400/16040]\n",
            "loss: 0.017546  [ 8000/16040]\n",
            "loss: 0.127730  [ 9600/16040]\n",
            "loss: 0.194040  [11200/16040]\n",
            "loss: 0.008644  [12800/16040]\n",
            "loss: 0.011890  [14400/16040]\n",
            "loss: 0.006792  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.5%, Avg loss: 0.026989 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.073760  [    0/16040]\n",
            "loss: 0.159040  [ 1600/16040]\n",
            "loss: 0.018191  [ 3200/16040]\n",
            "loss: 0.088129  [ 4800/16040]\n",
            "loss: 0.017536  [ 6400/16040]\n",
            "loss: 0.024050  [ 8000/16040]\n",
            "loss: 0.081887  [ 9600/16040]\n",
            "loss: 0.025039  [11200/16040]\n",
            "loss: 0.202575  [12800/16040]\n",
            "loss: 0.222987  [14400/16040]\n",
            "loss: 0.054387  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 96.3%, Avg loss: 0.102535 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.219290  [    0/16040]\n",
            "loss: 0.029772  [ 1600/16040]\n",
            "loss: 0.027394  [ 3200/16040]\n",
            "loss: 0.013235  [ 4800/16040]\n",
            "loss: 0.006681  [ 6400/16040]\n",
            "loss: 0.006071  [ 8000/16040]\n",
            "loss: 0.014234  [ 9600/16040]\n",
            "loss: 0.155486  [11200/16040]\n",
            "loss: 0.249083  [12800/16040]\n",
            "loss: 0.004723  [14400/16040]\n",
            "loss: 0.010640  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.8%, Avg loss: 0.023762 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.009564  [    0/16040]\n",
            "loss: 0.076535  [ 1600/16040]\n",
            "loss: 0.060475  [ 3200/16040]\n",
            "loss: 0.030545  [ 4800/16040]\n",
            "loss: 0.002862  [ 6400/16040]\n",
            "loss: 0.027715  [ 8000/16040]\n",
            "loss: 0.003212  [ 9600/16040]\n",
            "loss: 0.064770  [11200/16040]\n",
            "loss: 0.012020  [12800/16040]\n",
            "loss: 0.241609  [14400/16040]\n",
            "loss: 0.131977  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.5%, Avg loss: 0.174145 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.225156  [    0/16040]\n",
            "loss: 0.030904  [ 1600/16040]\n",
            "loss: 0.007432  [ 3200/16040]\n",
            "loss: 0.023325  [ 4800/16040]\n",
            "loss: 0.003519  [ 6400/16040]\n",
            "loss: 0.004035  [ 8000/16040]\n",
            "loss: 0.149559  [ 9600/16040]\n",
            "loss: 0.339395  [11200/16040]\n",
            "loss: 0.009696  [12800/16040]\n",
            "loss: 0.008305  [14400/16040]\n",
            "loss: 0.040910  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.204138 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.081348  [    0/16040]\n",
            "loss: 0.019826  [ 1600/16040]\n",
            "loss: 0.009447  [ 3200/16040]\n",
            "loss: 0.017087  [ 4800/16040]\n",
            "loss: 0.028001  [ 6400/16040]\n",
            "loss: 0.243687  [ 8000/16040]\n",
            "loss: 0.015700  [ 9600/16040]\n",
            "loss: 0.003082  [11200/16040]\n",
            "loss: 0.002636  [12800/16040]\n",
            "loss: 0.008061  [14400/16040]\n",
            "loss: 0.002275  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.006582 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.018182  [    0/16040]\n",
            "loss: 0.001077  [ 1600/16040]\n",
            "loss: 0.002085  [ 3200/16040]\n",
            "loss: 0.007900  [ 4800/16040]\n",
            "loss: 0.002712  [ 6400/16040]\n",
            "loss: 0.454339  [ 8000/16040]\n",
            "loss: 0.004635  [ 9600/16040]\n",
            "loss: 0.048797  [11200/16040]\n",
            "loss: 0.029735  [12800/16040]\n",
            "loss: 0.035326  [14400/16040]\n",
            "loss: 0.048151  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.071650 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Data = pd.read_csv('/Q3_test.csv', sep=',')\n",
        "Test = torch.tensor( data = Data.values, requires_grad = False ).int()\n",
        "Test_targets = torch.nn.functional.one_hot(Test[:,0].to(torch.int64))\n",
        "Test = torch.cat((Test[:,1:],Test_targets),1)\n",
        "Test_dataloader = DataLoader(Test, batch_size=16, shuffle=True)\n",
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "    print('\\n Accuracy of SGD without dropout layer on test data is:\\n')\n",
        "    test(Test_dataloader, model, loss_fn)\n",
        "    print('\\n Accuracy of ADAM without dropout layer on test data is:\\n')\n",
        "    test(Test_dataloader, model2, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aErdqJpiStmx",
        "outputId": "bab4f505-a410-4a26-a432-83c1cb0faf48"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy of SGD without dropout layer on test data is:\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 1.079017 \n",
            "\n",
            "\n",
            " Accuracy of ADAM without dropout layer on test data is:\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 73.0%, Avg loss: 1.782823 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part c):"
      ],
      "metadata": {
        "id": "hv3mtOJzV7AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.Dropout(),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 25)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model3 = NeuralNetwork2().to(device).float()\n",
        "print(model3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model3.parameters(), lr=8e-2)\n",
        "\n",
        "train_dataloader = DataLoader(Train, batch_size=16, shuffle=True)\n",
        "valid_dataloader = DataLoader(Valid, batch_size=16, shuffle=True)\n",
        "\n",
        "epochs = 60\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model3, loss_fn, optimizer)\n",
        "    test(valid_dataloader, model3, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz4nm5yRVvRe",
        "outputId": "f588aadd-200d-4451-8e27-841e2acbce9f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork2(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): ReLU()\n",
            "    (7): Linear(in_features=128, out_features=25, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.226254  [    0/16040]\n",
            "loss: 3.221869  [ 1600/16040]\n",
            "loss: 3.216150  [ 3200/16040]\n",
            "loss: 3.193453  [ 4800/16040]\n",
            "loss: 3.189657  [ 6400/16040]\n",
            "loss: 3.202899  [ 8000/16040]\n",
            "loss: 3.159932  [ 9600/16040]\n",
            "loss: 3.190446  [11200/16040]\n",
            "loss: 3.144313  [12800/16040]\n",
            "loss: 3.200562  [14400/16040]\n",
            "loss: 3.184952  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 4.5%, Avg loss: 3.183746 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 3.216225  [    0/16040]\n",
            "loss: 3.229225  [ 1600/16040]\n",
            "loss: 3.162158  [ 3200/16040]\n",
            "loss: 3.203601  [ 4800/16040]\n",
            "loss: 3.182652  [ 6400/16040]\n",
            "loss: 3.131972  [ 8000/16040]\n",
            "loss: 3.177982  [ 9600/16040]\n",
            "loss: 3.156120  [11200/16040]\n",
            "loss: 3.193924  [12800/16040]\n",
            "loss: 3.194800  [14400/16040]\n",
            "loss: 3.182282  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 4.4%, Avg loss: 3.179087 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 3.151799  [    0/16040]\n",
            "loss: 3.195641  [ 1600/16040]\n",
            "loss: 3.223754  [ 3200/16040]\n",
            "loss: 3.170630  [ 4800/16040]\n",
            "loss: 3.202411  [ 6400/16040]\n",
            "loss: 3.196365  [ 8000/16040]\n",
            "loss: 3.154623  [ 9600/16040]\n",
            "loss: 3.171107  [11200/16040]\n",
            "loss: 3.164799  [12800/16040]\n",
            "loss: 3.176953  [14400/16040]\n",
            "loss: 3.152796  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 4.3%, Avg loss: 3.177991 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 3.192161  [    0/16040]\n",
            "loss: 3.174240  [ 1600/16040]\n",
            "loss: 3.189613  [ 3200/16040]\n",
            "loss: 3.180186  [ 4800/16040]\n",
            "loss: 3.269953  [ 6400/16040]\n",
            "loss: 3.160657  [ 8000/16040]\n",
            "loss: 3.174178  [ 9600/16040]\n",
            "loss: 3.186164  [11200/16040]\n",
            "loss: 3.194481  [12800/16040]\n",
            "loss: 3.199121  [14400/16040]\n",
            "loss: 3.225196  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 4.9%, Avg loss: 3.176256 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 3.192923  [    0/16040]\n",
            "loss: 3.155392  [ 1600/16040]\n",
            "loss: 3.169137  [ 3200/16040]\n",
            "loss: 3.155187  [ 4800/16040]\n",
            "loss: 3.148236  [ 6400/16040]\n",
            "loss: 3.148427  [ 8000/16040]\n",
            "loss: 3.160908  [ 9600/16040]\n",
            "loss: 3.148513  [11200/16040]\n",
            "loss: 3.176614  [12800/16040]\n",
            "loss: 3.157738  [14400/16040]\n",
            "loss: 3.199967  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 5.9%, Avg loss: 3.164547 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 3.142573  [    0/16040]\n",
            "loss: 3.123187  [ 1600/16040]\n",
            "loss: 3.127992  [ 3200/16040]\n",
            "loss: 3.060648  [ 4800/16040]\n",
            "loss: 3.027051  [ 6400/16040]\n",
            "loss: 3.045398  [ 8000/16040]\n",
            "loss: 3.001811  [ 9600/16040]\n",
            "loss: 2.995476  [11200/16040]\n",
            "loss: 3.057491  [12800/16040]\n",
            "loss: 2.734443  [14400/16040]\n",
            "loss: 3.020504  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 12.2%, Avg loss: 2.805432 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.978104  [    0/16040]\n",
            "loss: 3.055156  [ 1600/16040]\n",
            "loss: 3.082346  [ 3200/16040]\n",
            "loss: 2.880857  [ 4800/16040]\n",
            "loss: 2.899565  [ 6400/16040]\n",
            "loss: 3.148625  [ 8000/16040]\n",
            "loss: 2.496345  [ 9600/16040]\n",
            "loss: 2.874360  [11200/16040]\n",
            "loss: 2.507332  [12800/16040]\n",
            "loss: 2.908132  [14400/16040]\n",
            "loss: 2.402281  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 11.1%, Avg loss: 2.865871 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 3.042379  [    0/16040]\n",
            "loss: 2.717775  [ 1600/16040]\n",
            "loss: 2.876421  [ 3200/16040]\n",
            "loss: 2.851058  [ 4800/16040]\n",
            "loss: 2.724042  [ 6400/16040]\n",
            "loss: 2.798499  [ 8000/16040]\n",
            "loss: 2.820439  [ 9600/16040]\n",
            "loss: 2.630749  [11200/16040]\n",
            "loss: 2.363040  [12800/16040]\n",
            "loss: 2.423647  [14400/16040]\n",
            "loss: 2.925854  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 13.4%, Avg loss: 2.679172 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 2.906334  [    0/16040]\n",
            "loss: 2.567144  [ 1600/16040]\n",
            "loss: 2.607940  [ 3200/16040]\n",
            "loss: 2.647968  [ 4800/16040]\n",
            "loss: 2.679014  [ 6400/16040]\n",
            "loss: 2.702575  [ 8000/16040]\n",
            "loss: 2.509133  [ 9600/16040]\n",
            "loss: 2.441529  [11200/16040]\n",
            "loss: 2.835353  [12800/16040]\n",
            "loss: 2.493931  [14400/16040]\n",
            "loss: 2.673827  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 17.5%, Avg loss: 2.455579 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.663825  [    0/16040]\n",
            "loss: 2.730334  [ 1600/16040]\n",
            "loss: 2.250564  [ 3200/16040]\n",
            "loss: 2.595780  [ 4800/16040]\n",
            "loss: 2.253623  [ 6400/16040]\n",
            "loss: 2.369386  [ 8000/16040]\n",
            "loss: 2.214815  [ 9600/16040]\n",
            "loss: 1.989247  [11200/16040]\n",
            "loss: 1.964897  [12800/16040]\n",
            "loss: 2.414243  [14400/16040]\n",
            "loss: 2.244621  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 25.7%, Avg loss: 2.173615 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.105212  [    0/16040]\n",
            "loss: 2.334743  [ 1600/16040]\n",
            "loss: 2.559947  [ 3200/16040]\n",
            "loss: 2.421246  [ 4800/16040]\n",
            "loss: 2.170597  [ 6400/16040]\n",
            "loss: 2.697716  [ 8000/16040]\n",
            "loss: 2.611544  [ 9600/16040]\n",
            "loss: 2.249272  [11200/16040]\n",
            "loss: 2.295935  [12800/16040]\n",
            "loss: 1.692069  [14400/16040]\n",
            "loss: 2.443440  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 21.2%, Avg loss: 2.362363 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.927500  [    0/16040]\n",
            "loss: 2.216228  [ 1600/16040]\n",
            "loss: 2.077527  [ 3200/16040]\n",
            "loss: 2.070065  [ 4800/16040]\n",
            "loss: 2.148085  [ 6400/16040]\n",
            "loss: 2.145516  [ 8000/16040]\n",
            "loss: 2.314743  [ 9600/16040]\n",
            "loss: 1.958519  [11200/16040]\n",
            "loss: 2.095829  [12800/16040]\n",
            "loss: 1.748132  [14400/16040]\n",
            "loss: 2.026074  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 17.5%, Avg loss: 2.414485 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.248349  [    0/16040]\n",
            "loss: 2.103637  [ 1600/16040]\n",
            "loss: 2.320901  [ 3200/16040]\n",
            "loss: 1.919464  [ 4800/16040]\n",
            "loss: 2.002795  [ 6400/16040]\n",
            "loss: 2.223634  [ 8000/16040]\n",
            "loss: 2.460323  [ 9600/16040]\n",
            "loss: 2.307658  [11200/16040]\n",
            "loss: 1.910429  [12800/16040]\n",
            "loss: 2.212543  [14400/16040]\n",
            "loss: 2.348154  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 22.7%, Avg loss: 2.210735 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.718258  [    0/16040]\n",
            "loss: 2.054904  [ 1600/16040]\n",
            "loss: 2.596022  [ 3200/16040]\n",
            "loss: 2.354311  [ 4800/16040]\n",
            "loss: 1.782731  [ 6400/16040]\n",
            "loss: 1.983267  [ 8000/16040]\n",
            "loss: 1.926968  [ 9600/16040]\n",
            "loss: 2.154422  [11200/16040]\n",
            "loss: 1.892170  [12800/16040]\n",
            "loss: 1.861425  [14400/16040]\n",
            "loss: 2.289655  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 25.1%, Avg loss: 2.155157 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 3.067318  [    0/16040]\n",
            "loss: 1.765500  [ 1600/16040]\n",
            "loss: 2.146430  [ 3200/16040]\n",
            "loss: 2.301711  [ 4800/16040]\n",
            "loss: 1.681459  [ 6400/16040]\n",
            "loss: 1.796186  [ 8000/16040]\n",
            "loss: 2.159666  [ 9600/16040]\n",
            "loss: 2.066633  [11200/16040]\n",
            "loss: 2.197799  [12800/16040]\n",
            "loss: 1.841163  [14400/16040]\n",
            "loss: 1.726199  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 41.2%, Avg loss: 1.690959 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.308318  [    0/16040]\n",
            "loss: 1.735224  [ 1600/16040]\n",
            "loss: 2.396339  [ 3200/16040]\n",
            "loss: 1.998337  [ 4800/16040]\n",
            "loss: 2.890659  [ 6400/16040]\n",
            "loss: 1.895327  [ 8000/16040]\n",
            "loss: 2.018456  [ 9600/16040]\n",
            "loss: 1.642224  [11200/16040]\n",
            "loss: 1.954573  [12800/16040]\n",
            "loss: 1.339849  [14400/16040]\n",
            "loss: 1.902954  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 44.2%, Avg loss: 1.611434 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 1.882215  [    0/16040]\n",
            "loss: 1.597068  [ 1600/16040]\n",
            "loss: 2.270773  [ 3200/16040]\n",
            "loss: 1.618873  [ 4800/16040]\n",
            "loss: 1.383192  [ 6400/16040]\n",
            "loss: 2.205394  [ 8000/16040]\n",
            "loss: 1.402506  [ 9600/16040]\n",
            "loss: 1.981766  [11200/16040]\n",
            "loss: 1.806260  [12800/16040]\n",
            "loss: 1.207195  [14400/16040]\n",
            "loss: 1.659571  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 28.8%, Avg loss: 2.355239 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1.693490  [    0/16040]\n",
            "loss: 1.078894  [ 1600/16040]\n",
            "loss: 1.509193  [ 3200/16040]\n",
            "loss: 1.661111  [ 4800/16040]\n",
            "loss: 1.540629  [ 6400/16040]\n",
            "loss: 1.501506  [ 8000/16040]\n",
            "loss: 1.714878  [ 9600/16040]\n",
            "loss: 1.450262  [11200/16040]\n",
            "loss: 1.584260  [12800/16040]\n",
            "loss: 1.739579  [14400/16040]\n",
            "loss: 1.464051  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 26.3%, Avg loss: 2.619168 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.399554  [    0/16040]\n",
            "loss: 1.217400  [ 1600/16040]\n",
            "loss: 1.990673  [ 3200/16040]\n",
            "loss: 1.613819  [ 4800/16040]\n",
            "loss: 1.327284  [ 6400/16040]\n",
            "loss: 1.592680  [ 8000/16040]\n",
            "loss: 1.497598  [ 9600/16040]\n",
            "loss: 1.030857  [11200/16040]\n",
            "loss: 2.027665  [12800/16040]\n",
            "loss: 2.038672  [14400/16040]\n",
            "loss: 1.462512  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 52.8%, Avg loss: 1.366394 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1.912503  [    0/16040]\n",
            "loss: 1.479045  [ 1600/16040]\n",
            "loss: 1.937655  [ 3200/16040]\n",
            "loss: 1.506771  [ 4800/16040]\n",
            "loss: 1.807940  [ 6400/16040]\n",
            "loss: 1.779100  [ 8000/16040]\n",
            "loss: 1.490512  [ 9600/16040]\n",
            "loss: 1.460579  [11200/16040]\n",
            "loss: 1.499675  [12800/16040]\n",
            "loss: 1.482555  [14400/16040]\n",
            "loss: 1.062559  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 44.3%, Avg loss: 1.572045 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.702513  [    0/16040]\n",
            "loss: 1.450994  [ 1600/16040]\n",
            "loss: 1.296178  [ 3200/16040]\n",
            "loss: 2.137444  [ 4800/16040]\n",
            "loss: 1.166822  [ 6400/16040]\n",
            "loss: 1.761389  [ 8000/16040]\n",
            "loss: 1.471418  [ 9600/16040]\n",
            "loss: 1.468656  [11200/16040]\n",
            "loss: 1.768056  [12800/16040]\n",
            "loss: 2.667637  [14400/16040]\n",
            "loss: 1.856248  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 52.8%, Avg loss: 1.258989 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1.924937  [    0/16040]\n",
            "loss: 0.559208  [ 1600/16040]\n",
            "loss: 3.000092  [ 3200/16040]\n",
            "loss: 1.908680  [ 4800/16040]\n",
            "loss: 1.081587  [ 6400/16040]\n",
            "loss: 1.156055  [ 8000/16040]\n",
            "loss: 1.195606  [ 9600/16040]\n",
            "loss: 1.975328  [11200/16040]\n",
            "loss: 1.145478  [12800/16040]\n",
            "loss: 2.262178  [14400/16040]\n",
            "loss: 1.234766  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 51.3%, Avg loss: 1.328115 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.920435  [    0/16040]\n",
            "loss: 1.572184  [ 1600/16040]\n",
            "loss: 1.062513  [ 3200/16040]\n",
            "loss: 1.431039  [ 4800/16040]\n",
            "loss: 1.251359  [ 6400/16040]\n",
            "loss: 0.965401  [ 8000/16040]\n",
            "loss: 1.114283  [ 9600/16040]\n",
            "loss: 1.019905  [11200/16040]\n",
            "loss: 3.275989  [12800/16040]\n",
            "loss: 1.326656  [14400/16040]\n",
            "loss: 1.127727  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 56.6%, Avg loss: 1.196504 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.985114  [    0/16040]\n",
            "loss: 2.223707  [ 1600/16040]\n",
            "loss: 1.011429  [ 3200/16040]\n",
            "loss: 1.228364  [ 4800/16040]\n",
            "loss: 1.139940  [ 6400/16040]\n",
            "loss: 1.250538  [ 8000/16040]\n",
            "loss: 1.137906  [ 9600/16040]\n",
            "loss: 1.193548  [11200/16040]\n",
            "loss: 0.663688  [12800/16040]\n",
            "loss: 2.117031  [14400/16040]\n",
            "loss: 1.194514  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 48.5%, Avg loss: 1.367799 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.976526  [    0/16040]\n",
            "loss: 1.279248  [ 1600/16040]\n",
            "loss: 1.192500  [ 3200/16040]\n",
            "loss: 0.906488  [ 4800/16040]\n",
            "loss: 0.986998  [ 6400/16040]\n",
            "loss: 0.986832  [ 8000/16040]\n",
            "loss: 0.938520  [ 9600/16040]\n",
            "loss: 1.191712  [11200/16040]\n",
            "loss: 0.918573  [12800/16040]\n",
            "loss: 1.226451  [14400/16040]\n",
            "loss: 1.067444  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 70.8%, Avg loss: 0.896543 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.931424  [    0/16040]\n",
            "loss: 1.363966  [ 1600/16040]\n",
            "loss: 0.794146  [ 3200/16040]\n",
            "loss: 0.544116  [ 4800/16040]\n",
            "loss: 1.602236  [ 6400/16040]\n",
            "loss: 1.217720  [ 8000/16040]\n",
            "loss: 0.994766  [ 9600/16040]\n",
            "loss: 0.956572  [11200/16040]\n",
            "loss: 1.348168  [12800/16040]\n",
            "loss: 1.147593  [14400/16040]\n",
            "loss: 0.606375  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 63.9%, Avg loss: 0.984833 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.688106  [    0/16040]\n",
            "loss: 0.844558  [ 1600/16040]\n",
            "loss: 0.799980  [ 3200/16040]\n",
            "loss: 0.924609  [ 4800/16040]\n",
            "loss: 1.199841  [ 6400/16040]\n",
            "loss: 0.903775  [ 8000/16040]\n",
            "loss: 1.611240  [ 9600/16040]\n",
            "loss: 1.558151  [11200/16040]\n",
            "loss: 1.175841  [12800/16040]\n",
            "loss: 1.107701  [14400/16040]\n",
            "loss: 1.330116  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 76.3%, Avg loss: 0.706489 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.631986  [    0/16040]\n",
            "loss: 0.828551  [ 1600/16040]\n",
            "loss: 1.122005  [ 3200/16040]\n",
            "loss: 1.018470  [ 4800/16040]\n",
            "loss: 1.600895  [ 6400/16040]\n",
            "loss: 1.171552  [ 8000/16040]\n",
            "loss: 1.241515  [ 9600/16040]\n",
            "loss: 0.771507  [11200/16040]\n",
            "loss: 0.945457  [12800/16040]\n",
            "loss: 1.288271  [14400/16040]\n",
            "loss: 0.857579  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 59.5%, Avg loss: 1.106602 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.967871  [    0/16040]\n",
            "loss: 0.533635  [ 1600/16040]\n",
            "loss: 0.832791  [ 3200/16040]\n",
            "loss: 0.964736  [ 4800/16040]\n",
            "loss: 1.058945  [ 6400/16040]\n",
            "loss: 0.538580  [ 8000/16040]\n",
            "loss: 0.950755  [ 9600/16040]\n",
            "loss: 1.011829  [11200/16040]\n",
            "loss: 0.985771  [12800/16040]\n",
            "loss: 0.965882  [14400/16040]\n",
            "loss: 0.885688  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 76.0%, Avg loss: 0.705579 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.448320  [    0/16040]\n",
            "loss: 1.134189  [ 1600/16040]\n",
            "loss: 0.889081  [ 3200/16040]\n",
            "loss: 0.986572  [ 4800/16040]\n",
            "loss: 0.632611  [ 6400/16040]\n",
            "loss: 0.683160  [ 8000/16040]\n",
            "loss: 1.027121  [ 9600/16040]\n",
            "loss: 0.870898  [11200/16040]\n",
            "loss: 1.176593  [12800/16040]\n",
            "loss: 1.293944  [14400/16040]\n",
            "loss: 0.673955  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 74.9%, Avg loss: 0.676387 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.696285  [    0/16040]\n",
            "loss: 0.535806  [ 1600/16040]\n",
            "loss: 0.661019  [ 3200/16040]\n",
            "loss: 0.892478  [ 4800/16040]\n",
            "loss: 1.436252  [ 6400/16040]\n",
            "loss: 0.224944  [ 8000/16040]\n",
            "loss: 0.937281  [ 9600/16040]\n",
            "loss: 0.821888  [11200/16040]\n",
            "loss: 0.924994  [12800/16040]\n",
            "loss: 0.650603  [14400/16040]\n",
            "loss: 0.485275  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.797615 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.583564  [    0/16040]\n",
            "loss: 0.904948  [ 1600/16040]\n",
            "loss: 0.590242  [ 3200/16040]\n",
            "loss: 0.979456  [ 4800/16040]\n",
            "loss: 0.268895  [ 6400/16040]\n",
            "loss: 0.600665  [ 8000/16040]\n",
            "loss: 0.777551  [ 9600/16040]\n",
            "loss: 2.050718  [11200/16040]\n",
            "loss: 0.344819  [12800/16040]\n",
            "loss: 0.389968  [14400/16040]\n",
            "loss: 0.416635  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.557034 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.838880  [    0/16040]\n",
            "loss: 0.599666  [ 1600/16040]\n",
            "loss: 0.816701  [ 3200/16040]\n",
            "loss: 0.585486  [ 4800/16040]\n",
            "loss: 1.007575  [ 6400/16040]\n",
            "loss: 0.541382  [ 8000/16040]\n",
            "loss: 0.847176  [ 9600/16040]\n",
            "loss: 0.877354  [11200/16040]\n",
            "loss: 1.052213  [12800/16040]\n",
            "loss: 0.508128  [14400/16040]\n",
            "loss: 0.647922  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 82.2%, Avg loss: 0.503264 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.574783  [    0/16040]\n",
            "loss: 0.366854  [ 1600/16040]\n",
            "loss: 0.441678  [ 3200/16040]\n",
            "loss: 0.847996  [ 4800/16040]\n",
            "loss: 0.276402  [ 6400/16040]\n",
            "loss: 0.683838  [ 8000/16040]\n",
            "loss: 0.554005  [ 9600/16040]\n",
            "loss: 0.322827  [11200/16040]\n",
            "loss: 1.954393  [12800/16040]\n",
            "loss: 0.390809  [14400/16040]\n",
            "loss: 1.950011  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.531718 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.806967  [    0/16040]\n",
            "loss: 0.398954  [ 1600/16040]\n",
            "loss: 0.130681  [ 3200/16040]\n",
            "loss: 0.698199  [ 4800/16040]\n",
            "loss: 0.435225  [ 6400/16040]\n",
            "loss: 0.686940  [ 8000/16040]\n",
            "loss: 0.545931  [ 9600/16040]\n",
            "loss: 1.749847  [11200/16040]\n",
            "loss: 0.493499  [12800/16040]\n",
            "loss: 1.644685  [14400/16040]\n",
            "loss: 0.858717  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 54.3%, Avg loss: 1.424408 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 1.101592  [    0/16040]\n",
            "loss: 0.407385  [ 1600/16040]\n",
            "loss: 0.268577  [ 3200/16040]\n",
            "loss: 0.858152  [ 4800/16040]\n",
            "loss: 0.634012  [ 6400/16040]\n",
            "loss: 0.413716  [ 8000/16040]\n",
            "loss: 0.861615  [ 9600/16040]\n",
            "loss: 0.793420  [11200/16040]\n",
            "loss: 1.009853  [12800/16040]\n",
            "loss: 0.574684  [14400/16040]\n",
            "loss: 0.336311  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.519321 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 0.722866  [    0/16040]\n",
            "loss: 0.530142  [ 1600/16040]\n",
            "loss: 0.793027  [ 3200/16040]\n",
            "loss: 0.304337  [ 4800/16040]\n",
            "loss: 0.390013  [ 6400/16040]\n",
            "loss: 0.728834  [ 8000/16040]\n",
            "loss: 0.448196  [ 9600/16040]\n",
            "loss: 0.515290  [11200/16040]\n",
            "loss: 0.610757  [12800/16040]\n",
            "loss: 0.858664  [14400/16040]\n",
            "loss: 0.489825  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 85.5%, Avg loss: 0.407519 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.709012  [    0/16040]\n",
            "loss: 0.595853  [ 1600/16040]\n",
            "loss: 0.324639  [ 3200/16040]\n",
            "loss: 0.837532  [ 4800/16040]\n",
            "loss: 0.642612  [ 6400/16040]\n",
            "loss: 0.310219  [ 8000/16040]\n",
            "loss: 0.699171  [ 9600/16040]\n",
            "loss: 0.456383  [11200/16040]\n",
            "loss: 0.479137  [12800/16040]\n",
            "loss: 0.321620  [14400/16040]\n",
            "loss: 0.349168  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 87.8%, Avg loss: 0.342839 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.191658  [    0/16040]\n",
            "loss: 0.585193  [ 1600/16040]\n",
            "loss: 0.576493  [ 3200/16040]\n",
            "loss: 0.508817  [ 4800/16040]\n",
            "loss: 0.053063  [ 6400/16040]\n",
            "loss: 0.185919  [ 8000/16040]\n",
            "loss: 0.293840  [ 9600/16040]\n",
            "loss: 0.821996  [11200/16040]\n",
            "loss: 0.285109  [12800/16040]\n",
            "loss: 0.316558  [14400/16040]\n",
            "loss: 0.372512  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 82.0%, Avg loss: 0.509090 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.577843  [    0/16040]\n",
            "loss: 0.418438  [ 1600/16040]\n",
            "loss: 2.003251  [ 3200/16040]\n",
            "loss: 0.457722  [ 4800/16040]\n",
            "loss: 0.163463  [ 6400/16040]\n",
            "loss: 0.045268  [ 8000/16040]\n",
            "loss: 0.353757  [ 9600/16040]\n",
            "loss: 0.347216  [11200/16040]\n",
            "loss: 0.306696  [12800/16040]\n",
            "loss: 1.257559  [14400/16040]\n",
            "loss: 0.113331  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 90.7%, Avg loss: 0.284526 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.289501  [    0/16040]\n",
            "loss: 0.666590  [ 1600/16040]\n",
            "loss: 0.052145  [ 3200/16040]\n",
            "loss: 1.711096  [ 4800/16040]\n",
            "loss: 0.388883  [ 6400/16040]\n",
            "loss: 0.428311  [ 8000/16040]\n",
            "loss: 0.419185  [ 9600/16040]\n",
            "loss: 0.702676  [11200/16040]\n",
            "loss: 0.443017  [12800/16040]\n",
            "loss: 0.311095  [14400/16040]\n",
            "loss: 0.191237  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.2%, Avg loss: 0.568250 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.528986  [    0/16040]\n",
            "loss: 1.682818  [ 1600/16040]\n",
            "loss: 0.317623  [ 3200/16040]\n",
            "loss: 0.286947  [ 4800/16040]\n",
            "loss: 0.506490  [ 6400/16040]\n",
            "loss: 0.312018  [ 8000/16040]\n",
            "loss: 0.534505  [ 9600/16040]\n",
            "loss: 0.462873  [11200/16040]\n",
            "loss: 0.316462  [12800/16040]\n",
            "loss: 0.199958  [14400/16040]\n",
            "loss: 0.193803  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.6%, Avg loss: 0.203132 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.346758  [    0/16040]\n",
            "loss: 0.168641  [ 1600/16040]\n",
            "loss: 0.120025  [ 3200/16040]\n",
            "loss: 0.071141  [ 4800/16040]\n",
            "loss: 0.289733  [ 6400/16040]\n",
            "loss: 0.296724  [ 8000/16040]\n",
            "loss: 0.324201  [ 9600/16040]\n",
            "loss: 0.636504  [11200/16040]\n",
            "loss: 0.307172  [12800/16040]\n",
            "loss: 0.151060  [14400/16040]\n",
            "loss: 0.379105  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.192788 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.347763  [    0/16040]\n",
            "loss: 0.202936  [ 1600/16040]\n",
            "loss: 0.083606  [ 3200/16040]\n",
            "loss: 0.443565  [ 4800/16040]\n",
            "loss: 0.287589  [ 6400/16040]\n",
            "loss: 0.201574  [ 8000/16040]\n",
            "loss: 0.448761  [ 9600/16040]\n",
            "loss: 0.331861  [11200/16040]\n",
            "loss: 0.330396  [12800/16040]\n",
            "loss: 0.146869  [14400/16040]\n",
            "loss: 0.288826  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.0%, Avg loss: 0.207281 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.380043  [    0/16040]\n",
            "loss: 0.384145  [ 1600/16040]\n",
            "loss: 0.415464  [ 3200/16040]\n",
            "loss: 0.372001  [ 4800/16040]\n",
            "loss: 0.726111  [ 6400/16040]\n",
            "loss: 0.212344  [ 8000/16040]\n",
            "loss: 1.109276  [ 9600/16040]\n",
            "loss: 0.524037  [11200/16040]\n",
            "loss: 0.114796  [12800/16040]\n",
            "loss: 0.308200  [14400/16040]\n",
            "loss: 0.289950  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.3%, Avg loss: 0.177860 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.274269  [    0/16040]\n",
            "loss: 0.285921  [ 1600/16040]\n",
            "loss: 0.195239  [ 3200/16040]\n",
            "loss: 0.388410  [ 4800/16040]\n",
            "loss: 0.557468  [ 6400/16040]\n",
            "loss: 0.055706  [ 8000/16040]\n",
            "loss: 0.216824  [ 9600/16040]\n",
            "loss: 0.230092  [11200/16040]\n",
            "loss: 0.102837  [12800/16040]\n",
            "loss: 0.113941  [14400/16040]\n",
            "loss: 0.101976  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.158506 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.218087  [    0/16040]\n",
            "loss: 0.349782  [ 1600/16040]\n",
            "loss: 0.105267  [ 3200/16040]\n",
            "loss: 0.069778  [ 4800/16040]\n",
            "loss: 0.336691  [ 6400/16040]\n",
            "loss: 0.298986  [ 8000/16040]\n",
            "loss: 0.191493  [ 9600/16040]\n",
            "loss: 0.322668  [11200/16040]\n",
            "loss: 0.137521  [12800/16040]\n",
            "loss: 0.378206  [14400/16040]\n",
            "loss: 0.178567  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 84.0%, Avg loss: 0.485875 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.601772  [    0/16040]\n",
            "loss: 0.208705  [ 1600/16040]\n",
            "loss: 0.093758  [ 3200/16040]\n",
            "loss: 0.387145  [ 4800/16040]\n",
            "loss: 0.217598  [ 6400/16040]\n",
            "loss: 0.375689  [ 8000/16040]\n",
            "loss: 0.352528  [ 9600/16040]\n",
            "loss: 0.282979  [11200/16040]\n",
            "loss: 0.402045  [12800/16040]\n",
            "loss: 0.252426  [14400/16040]\n",
            "loss: 0.404724  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.8%, Avg loss: 0.181820 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.272748  [    0/16040]\n",
            "loss: 0.117727  [ 1600/16040]\n",
            "loss: 0.183273  [ 3200/16040]\n",
            "loss: 0.336570  [ 4800/16040]\n",
            "loss: 0.151123  [ 6400/16040]\n",
            "loss: 0.325679  [ 8000/16040]\n",
            "loss: 0.115094  [ 9600/16040]\n",
            "loss: 0.167328  [11200/16040]\n",
            "loss: 0.062517  [12800/16040]\n",
            "loss: 0.047662  [14400/16040]\n",
            "loss: 0.251077  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 95.5%, Avg loss: 0.149042 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.245546  [    0/16040]\n",
            "loss: 0.592485  [ 1600/16040]\n",
            "loss: 0.127790  [ 3200/16040]\n",
            "loss: 0.139376  [ 4800/16040]\n",
            "loss: 0.092468  [ 6400/16040]\n",
            "loss: 0.062660  [ 8000/16040]\n",
            "loss: 0.210573  [ 9600/16040]\n",
            "loss: 0.128596  [11200/16040]\n",
            "loss: 0.465262  [12800/16040]\n",
            "loss: 0.199679  [14400/16040]\n",
            "loss: 0.201522  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.2%, Avg loss: 0.117545 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.344981  [    0/16040]\n",
            "loss: 0.050639  [ 1600/16040]\n",
            "loss: 0.057024  [ 3200/16040]\n",
            "loss: 1.328622  [ 4800/16040]\n",
            "loss: 0.133773  [ 6400/16040]\n",
            "loss: 0.486453  [ 8000/16040]\n",
            "loss: 0.170568  [ 9600/16040]\n",
            "loss: 0.159546  [11200/16040]\n",
            "loss: 0.046233  [12800/16040]\n",
            "loss: 0.123549  [14400/16040]\n",
            "loss: 0.502035  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 36.7%, Avg loss: 5.373480 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 6.609630  [    0/16040]\n",
            "loss: 0.455279  [ 1600/16040]\n",
            "loss: 0.195240  [ 3200/16040]\n",
            "loss: 0.217860  [ 4800/16040]\n",
            "loss: 0.484903  [ 6400/16040]\n",
            "loss: 0.160266  [ 8000/16040]\n",
            "loss: 0.490353  [ 9600/16040]\n",
            "loss: 0.167371  [11200/16040]\n",
            "loss: 0.051662  [12800/16040]\n",
            "loss: 0.176021  [14400/16040]\n",
            "loss: 1.759964  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 42.9%, Avg loss: 4.509043 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 4.105488  [    0/16040]\n",
            "loss: 0.077743  [ 1600/16040]\n",
            "loss: 0.071970  [ 3200/16040]\n",
            "loss: 0.314886  [ 4800/16040]\n",
            "loss: 0.087393  [ 6400/16040]\n",
            "loss: 0.159521  [ 8000/16040]\n",
            "loss: 0.161395  [ 9600/16040]\n",
            "loss: 0.076308  [11200/16040]\n",
            "loss: 0.135431  [12800/16040]\n",
            "loss: 0.100539  [14400/16040]\n",
            "loss: 0.306602  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 73.0%, Avg loss: 1.018637 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 1.246174  [    0/16040]\n",
            "loss: 0.286723  [ 1600/16040]\n",
            "loss: 0.238497  [ 3200/16040]\n",
            "loss: 0.042774  [ 4800/16040]\n",
            "loss: 0.101822  [ 6400/16040]\n",
            "loss: 0.153547  [ 8000/16040]\n",
            "loss: 0.169417  [ 9600/16040]\n",
            "loss: 0.285304  [11200/16040]\n",
            "loss: 0.057999  [12800/16040]\n",
            "loss: 0.231650  [14400/16040]\n",
            "loss: 0.152565  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.058652 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.083284  [    0/16040]\n",
            "loss: 0.098790  [ 1600/16040]\n",
            "loss: 0.438619  [ 3200/16040]\n",
            "loss: 0.054496  [ 4800/16040]\n",
            "loss: 0.393243  [ 6400/16040]\n",
            "loss: 0.301522  [ 8000/16040]\n",
            "loss: 0.013584  [ 9600/16040]\n",
            "loss: 0.304569  [11200/16040]\n",
            "loss: 0.191338  [12800/16040]\n",
            "loss: 0.078528  [14400/16040]\n",
            "loss: 0.109883  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.3%, Avg loss: 0.064073 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.189678  [    0/16040]\n",
            "loss: 0.246076  [ 1600/16040]\n",
            "loss: 0.118839  [ 3200/16040]\n",
            "loss: 0.036385  [ 4800/16040]\n",
            "loss: 0.183428  [ 6400/16040]\n",
            "loss: 0.075523  [ 8000/16040]\n",
            "loss: 0.048943  [ 9600/16040]\n",
            "loss: 0.075259  [11200/16040]\n",
            "loss: 0.020449  [12800/16040]\n",
            "loss: 0.371161  [14400/16040]\n",
            "loss: 0.022436  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 86.4%, Avg loss: 0.384494 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.632370  [    0/16040]\n",
            "loss: 0.073186  [ 1600/16040]\n",
            "loss: 0.104040  [ 3200/16040]\n",
            "loss: 0.161386  [ 4800/16040]\n",
            "loss: 0.075186  [ 6400/16040]\n",
            "loss: 0.037086  [ 8000/16040]\n",
            "loss: 0.205192  [ 9600/16040]\n",
            "loss: 0.106033  [11200/16040]\n",
            "loss: 0.094580  [12800/16040]\n",
            "loss: 0.219230  [14400/16040]\n",
            "loss: 0.185921  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.075909 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.029459  [    0/16040]\n",
            "loss: 0.057588  [ 1600/16040]\n",
            "loss: 0.080716  [ 3200/16040]\n",
            "loss: 0.336797  [ 4800/16040]\n",
            "loss: 0.042805  [ 6400/16040]\n",
            "loss: 0.016078  [ 8000/16040]\n",
            "loss: 0.303400  [ 9600/16040]\n",
            "loss: 0.735380  [11200/16040]\n",
            "loss: 0.111163  [12800/16040]\n",
            "loss: 0.001693  [14400/16040]\n",
            "loss: 0.542462  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 89.6%, Avg loss: 0.276638 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.210166  [    0/16040]\n",
            "loss: 0.126782  [ 1600/16040]\n",
            "loss: 0.014376  [ 3200/16040]\n",
            "loss: 0.028249  [ 4800/16040]\n",
            "loss: 0.066109  [ 6400/16040]\n",
            "loss: 0.079794  [ 8000/16040]\n",
            "loss: 0.013662  [ 9600/16040]\n",
            "loss: 0.062544  [11200/16040]\n",
            "loss: 0.052981  [12800/16040]\n",
            "loss: 0.068786  [14400/16040]\n",
            "loss: 0.060257  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.044293 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.046598  [    0/16040]\n",
            "loss: 0.048499  [ 1600/16040]\n",
            "loss: 0.049022  [ 3200/16040]\n",
            "loss: 0.127671  [ 4800/16040]\n",
            "loss: 0.067164  [ 6400/16040]\n",
            "loss: 0.010347  [ 8000/16040]\n",
            "loss: 0.081700  [ 9600/16040]\n",
            "loss: 0.615139  [11200/16040]\n",
            "loss: 0.121895  [12800/16040]\n",
            "loss: 0.035413  [14400/16040]\n",
            "loss: 0.199083  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.046821 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = NeuralNetwork2().to(device).float()\n",
        "optimizer = torch.optim.Adam(model4.parameters(), lr=5e-4)\n",
        "optimizer.zero_grad()\n",
        "epochs = 60\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model4, loss_fn, optimizer)\n",
        "    test(valid_dataloader, model4, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ga_zB_WYXtO",
        "outputId": "caa79cef-bd61-480b-a08c-125f1d7b756f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 3.214543  [    0/16040]\n",
            "loss: 3.183165  [ 1600/16040]\n",
            "loss: 3.209741  [ 3200/16040]\n",
            "loss: 3.190517  [ 4800/16040]\n",
            "loss: 3.123632  [ 6400/16040]\n",
            "loss: 3.198734  [ 8000/16040]\n",
            "loss: 2.942446  [ 9600/16040]\n",
            "loss: 2.674212  [11200/16040]\n",
            "loss: 2.558357  [12800/16040]\n",
            "loss: 2.848825  [14400/16040]\n",
            "loss: 2.734051  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 16.0%, Avg loss: 2.580682 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.450885  [    0/16040]\n",
            "loss: 2.474297  [ 1600/16040]\n",
            "loss: 2.604660  [ 3200/16040]\n",
            "loss: 2.876366  [ 4800/16040]\n",
            "loss: 2.350549  [ 6400/16040]\n",
            "loss: 2.411824  [ 8000/16040]\n",
            "loss: 2.608701  [ 9600/16040]\n",
            "loss: 2.450571  [11200/16040]\n",
            "loss: 2.376332  [12800/16040]\n",
            "loss: 2.214232  [14400/16040]\n",
            "loss: 2.014428  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 27.9%, Avg loss: 2.102211 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.224745  [    0/16040]\n",
            "loss: 2.430825  [ 1600/16040]\n",
            "loss: 2.161049  [ 3200/16040]\n",
            "loss: 2.196945  [ 4800/16040]\n",
            "loss: 2.224064  [ 6400/16040]\n",
            "loss: 2.121095  [ 8000/16040]\n",
            "loss: 1.596093  [ 9600/16040]\n",
            "loss: 1.991067  [11200/16040]\n",
            "loss: 2.704931  [12800/16040]\n",
            "loss: 1.699382  [14400/16040]\n",
            "loss: 1.799255  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 36.3%, Avg loss: 1.825030 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.024151  [    0/16040]\n",
            "loss: 2.238799  [ 1600/16040]\n",
            "loss: 1.866462  [ 3200/16040]\n",
            "loss: 2.186555  [ 4800/16040]\n",
            "loss: 1.795347  [ 6400/16040]\n",
            "loss: 1.714935  [ 8000/16040]\n",
            "loss: 1.604906  [ 9600/16040]\n",
            "loss: 2.276282  [11200/16040]\n",
            "loss: 1.937458  [12800/16040]\n",
            "loss: 2.014716  [14400/16040]\n",
            "loss: 2.010364  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 39.7%, Avg loss: 1.672012 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.530701  [    0/16040]\n",
            "loss: 1.453938  [ 1600/16040]\n",
            "loss: 2.120824  [ 3200/16040]\n",
            "loss: 2.129598  [ 4800/16040]\n",
            "loss: 1.669316  [ 6400/16040]\n",
            "loss: 1.963056  [ 8000/16040]\n",
            "loss: 2.042779  [ 9600/16040]\n",
            "loss: 1.747275  [11200/16040]\n",
            "loss: 1.394080  [12800/16040]\n",
            "loss: 1.999214  [14400/16040]\n",
            "loss: 1.438968  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 43.8%, Avg loss: 1.547826 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.277367  [    0/16040]\n",
            "loss: 1.304122  [ 1600/16040]\n",
            "loss: 1.573457  [ 3200/16040]\n",
            "loss: 1.407856  [ 4800/16040]\n",
            "loss: 1.361859  [ 6400/16040]\n",
            "loss: 1.644731  [ 8000/16040]\n",
            "loss: 1.541456  [ 9600/16040]\n",
            "loss: 1.728158  [11200/16040]\n",
            "loss: 1.301552  [12800/16040]\n",
            "loss: 1.439710  [14400/16040]\n",
            "loss: 2.665687  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 47.9%, Avg loss: 1.434162 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.495496  [    0/16040]\n",
            "loss: 1.754347  [ 1600/16040]\n",
            "loss: 1.693079  [ 3200/16040]\n",
            "loss: 1.432988  [ 4800/16040]\n",
            "loss: 2.019078  [ 6400/16040]\n",
            "loss: 1.565390  [ 8000/16040]\n",
            "loss: 1.632024  [ 9600/16040]\n",
            "loss: 1.770715  [11200/16040]\n",
            "loss: 1.897936  [12800/16040]\n",
            "loss: 1.507032  [14400/16040]\n",
            "loss: 1.447995  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 50.5%, Avg loss: 1.358237 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.884614  [    0/16040]\n",
            "loss: 1.489009  [ 1600/16040]\n",
            "loss: 1.391148  [ 3200/16040]\n",
            "loss: 2.022375  [ 4800/16040]\n",
            "loss: 1.317670  [ 6400/16040]\n",
            "loss: 1.234826  [ 8000/16040]\n",
            "loss: 1.242098  [ 9600/16040]\n",
            "loss: 1.377769  [11200/16040]\n",
            "loss: 1.173235  [12800/16040]\n",
            "loss: 1.261721  [14400/16040]\n",
            "loss: 1.099331  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 50.1%, Avg loss: 1.295865 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.475214  [    0/16040]\n",
            "loss: 1.389276  [ 1600/16040]\n",
            "loss: 1.037445  [ 3200/16040]\n",
            "loss: 1.371207  [ 4800/16040]\n",
            "loss: 1.244188  [ 6400/16040]\n",
            "loss: 1.201113  [ 8000/16040]\n",
            "loss: 1.241233  [ 9600/16040]\n",
            "loss: 1.847167  [11200/16040]\n",
            "loss: 1.366370  [12800/16040]\n",
            "loss: 1.204256  [14400/16040]\n",
            "loss: 1.274770  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 57.4%, Avg loss: 1.170715 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.083449  [    0/16040]\n",
            "loss: 1.323560  [ 1600/16040]\n",
            "loss: 1.221023  [ 3200/16040]\n",
            "loss: 0.991407  [ 4800/16040]\n",
            "loss: 1.293063  [ 6400/16040]\n",
            "loss: 1.241720  [ 8000/16040]\n",
            "loss: 1.230576  [ 9600/16040]\n",
            "loss: 1.221179  [11200/16040]\n",
            "loss: 1.642930  [12800/16040]\n",
            "loss: 1.127134  [14400/16040]\n",
            "loss: 1.267716  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 61.2%, Avg loss: 1.086430 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1.268798  [    0/16040]\n",
            "loss: 1.135632  [ 1600/16040]\n",
            "loss: 1.244923  [ 3200/16040]\n",
            "loss: 1.069620  [ 4800/16040]\n",
            "loss: 1.502470  [ 6400/16040]\n",
            "loss: 1.715330  [ 8000/16040]\n",
            "loss: 1.392949  [ 9600/16040]\n",
            "loss: 1.188153  [11200/16040]\n",
            "loss: 1.373985  [12800/16040]\n",
            "loss: 0.919005  [14400/16040]\n",
            "loss: 1.136722  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 64.7%, Avg loss: 0.966228 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 1.110433  [    0/16040]\n",
            "loss: 1.077329  [ 1600/16040]\n",
            "loss: 1.395236  [ 3200/16040]\n",
            "loss: 1.363782  [ 4800/16040]\n",
            "loss: 1.031390  [ 6400/16040]\n",
            "loss: 1.476213  [ 8000/16040]\n",
            "loss: 1.423072  [ 9600/16040]\n",
            "loss: 1.215655  [11200/16040]\n",
            "loss: 1.227808  [12800/16040]\n",
            "loss: 1.147931  [14400/16040]\n",
            "loss: 1.494915  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 62.4%, Avg loss: 1.014802 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1.109324  [    0/16040]\n",
            "loss: 0.810861  [ 1600/16040]\n",
            "loss: 1.547246  [ 3200/16040]\n",
            "loss: 1.166291  [ 4800/16040]\n",
            "loss: 1.213421  [ 6400/16040]\n",
            "loss: 1.234759  [ 8000/16040]\n",
            "loss: 0.827527  [ 9600/16040]\n",
            "loss: 0.836055  [11200/16040]\n",
            "loss: 1.028645  [12800/16040]\n",
            "loss: 0.924514  [14400/16040]\n",
            "loss: 0.615196  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 65.6%, Avg loss: 0.923279 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 1.135495  [    0/16040]\n",
            "loss: 0.634780  [ 1600/16040]\n",
            "loss: 0.794033  [ 3200/16040]\n",
            "loss: 1.020557  [ 4800/16040]\n",
            "loss: 0.768240  [ 6400/16040]\n",
            "loss: 1.008901  [ 8000/16040]\n",
            "loss: 1.154440  [ 9600/16040]\n",
            "loss: 0.659405  [11200/16040]\n",
            "loss: 0.909654  [12800/16040]\n",
            "loss: 0.865773  [14400/16040]\n",
            "loss: 0.951185  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 72.0%, Avg loss: 0.801185 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 1.203923  [    0/16040]\n",
            "loss: 0.721072  [ 1600/16040]\n",
            "loss: 1.101153  [ 3200/16040]\n",
            "loss: 0.863431  [ 4800/16040]\n",
            "loss: 0.962328  [ 6400/16040]\n",
            "loss: 0.636665  [ 8000/16040]\n",
            "loss: 0.808215  [ 9600/16040]\n",
            "loss: 0.862370  [11200/16040]\n",
            "loss: 0.765762  [12800/16040]\n",
            "loss: 1.090351  [14400/16040]\n",
            "loss: 0.590302  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 74.2%, Avg loss: 0.716675 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1.325650  [    0/16040]\n",
            "loss: 1.287039  [ 1600/16040]\n",
            "loss: 0.950161  [ 3200/16040]\n",
            "loss: 0.600456  [ 4800/16040]\n",
            "loss: 0.921621  [ 6400/16040]\n",
            "loss: 1.168057  [ 8000/16040]\n",
            "loss: 1.067333  [ 9600/16040]\n",
            "loss: 1.019454  [11200/16040]\n",
            "loss: 1.042399  [12800/16040]\n",
            "loss: 0.697489  [14400/16040]\n",
            "loss: 1.258651  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 74.0%, Avg loss: 0.692586 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 0.828731  [    0/16040]\n",
            "loss: 0.770178  [ 1600/16040]\n",
            "loss: 1.140277  [ 3200/16040]\n",
            "loss: 0.562505  [ 4800/16040]\n",
            "loss: 0.770062  [ 6400/16040]\n",
            "loss: 0.976343  [ 8000/16040]\n",
            "loss: 1.172230  [ 9600/16040]\n",
            "loss: 0.681324  [11200/16040]\n",
            "loss: 0.467114  [12800/16040]\n",
            "loss: 0.441858  [14400/16040]\n",
            "loss: 0.947125  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 77.6%, Avg loss: 0.616056 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 0.777547  [    0/16040]\n",
            "loss: 1.608910  [ 1600/16040]\n",
            "loss: 0.837716  [ 3200/16040]\n",
            "loss: 0.778264  [ 4800/16040]\n",
            "loss: 0.846548  [ 6400/16040]\n",
            "loss: 0.517716  [ 8000/16040]\n",
            "loss: 0.565899  [ 9600/16040]\n",
            "loss: 0.543527  [11200/16040]\n",
            "loss: 0.569735  [12800/16040]\n",
            "loss: 1.117629  [14400/16040]\n",
            "loss: 1.055759  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 75.2%, Avg loss: 0.668945 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 1.008143  [    0/16040]\n",
            "loss: 0.713853  [ 1600/16040]\n",
            "loss: 0.704429  [ 3200/16040]\n",
            "loss: 0.734254  [ 4800/16040]\n",
            "loss: 0.939093  [ 6400/16040]\n",
            "loss: 0.844353  [ 8000/16040]\n",
            "loss: 0.610533  [ 9600/16040]\n",
            "loss: 0.661736  [11200/16040]\n",
            "loss: 0.550673  [12800/16040]\n",
            "loss: 0.666758  [14400/16040]\n",
            "loss: 0.580270  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.8%, Avg loss: 0.510020 \n",
            "\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 0.690421  [    0/16040]\n",
            "loss: 1.370165  [ 1600/16040]\n",
            "loss: 0.734015  [ 3200/16040]\n",
            "loss: 0.812620  [ 4800/16040]\n",
            "loss: 1.023902  [ 6400/16040]\n",
            "loss: 0.413241  [ 8000/16040]\n",
            "loss: 0.358898  [ 9600/16040]\n",
            "loss: 0.983024  [11200/16040]\n",
            "loss: 0.740534  [12800/16040]\n",
            "loss: 0.435191  [14400/16040]\n",
            "loss: 0.453040  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 70.5%, Avg loss: 0.730752 \n",
            "\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 1.079630  [    0/16040]\n",
            "loss: 0.648790  [ 1600/16040]\n",
            "loss: 0.565040  [ 3200/16040]\n",
            "loss: 0.559039  [ 4800/16040]\n",
            "loss: 0.525841  [ 6400/16040]\n",
            "loss: 0.786884  [ 8000/16040]\n",
            "loss: 0.924339  [ 9600/16040]\n",
            "loss: 0.923817  [11200/16040]\n",
            "loss: 0.595857  [12800/16040]\n",
            "loss: 0.476900  [14400/16040]\n",
            "loss: 0.616170  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.5%, Avg loss: 0.472092 \n",
            "\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 0.552367  [    0/16040]\n",
            "loss: 0.515631  [ 1600/16040]\n",
            "loss: 0.316750  [ 3200/16040]\n",
            "loss: 0.545284  [ 4800/16040]\n",
            "loss: 0.689637  [ 6400/16040]\n",
            "loss: 0.465830  [ 8000/16040]\n",
            "loss: 0.549024  [ 9600/16040]\n",
            "loss: 0.815960  [11200/16040]\n",
            "loss: 1.001734  [12800/16040]\n",
            "loss: 0.390461  [14400/16040]\n",
            "loss: 0.473644  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 86.1%, Avg loss: 0.387454 \n",
            "\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 0.353893  [    0/16040]\n",
            "loss: 0.737464  [ 1600/16040]\n",
            "loss: 0.506005  [ 3200/16040]\n",
            "loss: 0.666280  [ 4800/16040]\n",
            "loss: 0.888737  [ 6400/16040]\n",
            "loss: 0.362382  [ 8000/16040]\n",
            "loss: 0.617090  [ 9600/16040]\n",
            "loss: 0.795816  [11200/16040]\n",
            "loss: 0.725681  [12800/16040]\n",
            "loss: 0.231720  [14400/16040]\n",
            "loss: 0.351798  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 86.2%, Avg loss: 0.383439 \n",
            "\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 0.613868  [    0/16040]\n",
            "loss: 0.403575  [ 1600/16040]\n",
            "loss: 0.588239  [ 3200/16040]\n",
            "loss: 0.332025  [ 4800/16040]\n",
            "loss: 0.360239  [ 6400/16040]\n",
            "loss: 0.652129  [ 8000/16040]\n",
            "loss: 0.268193  [ 9600/16040]\n",
            "loss: 0.458813  [11200/16040]\n",
            "loss: 1.155225  [12800/16040]\n",
            "loss: 0.483765  [14400/16040]\n",
            "loss: 0.377438  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.0%, Avg loss: 0.490415 \n",
            "\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 0.831341  [    0/16040]\n",
            "loss: 0.257844  [ 1600/16040]\n",
            "loss: 0.660114  [ 3200/16040]\n",
            "loss: 0.116533  [ 4800/16040]\n",
            "loss: 0.411810  [ 6400/16040]\n",
            "loss: 0.716733  [ 8000/16040]\n",
            "loss: 0.234208  [ 9600/16040]\n",
            "loss: 0.653486  [11200/16040]\n",
            "loss: 0.627377  [12800/16040]\n",
            "loss: 0.453435  [14400/16040]\n",
            "loss: 0.270478  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 84.2%, Avg loss: 0.407166 \n",
            "\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 0.345473  [    0/16040]\n",
            "loss: 0.227094  [ 1600/16040]\n",
            "loss: 0.426330  [ 3200/16040]\n",
            "loss: 0.264777  [ 4800/16040]\n",
            "loss: 0.376605  [ 6400/16040]\n",
            "loss: 0.471728  [ 8000/16040]\n",
            "loss: 0.267086  [ 9600/16040]\n",
            "loss: 0.311415  [11200/16040]\n",
            "loss: 0.209776  [12800/16040]\n",
            "loss: 0.316044  [14400/16040]\n",
            "loss: 0.774279  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 86.7%, Avg loss: 0.365700 \n",
            "\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 0.643540  [    0/16040]\n",
            "loss: 0.060985  [ 1600/16040]\n",
            "loss: 0.576109  [ 3200/16040]\n",
            "loss: 0.256320  [ 4800/16040]\n",
            "loss: 0.436752  [ 6400/16040]\n",
            "loss: 0.437487  [ 8000/16040]\n",
            "loss: 0.591997  [ 9600/16040]\n",
            "loss: 0.658992  [11200/16040]\n",
            "loss: 0.382533  [12800/16040]\n",
            "loss: 0.224279  [14400/16040]\n",
            "loss: 0.942368  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 90.5%, Avg loss: 0.257308 \n",
            "\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 0.353863  [    0/16040]\n",
            "loss: 0.822267  [ 1600/16040]\n",
            "loss: 0.306000  [ 3200/16040]\n",
            "loss: 0.253502  [ 4800/16040]\n",
            "loss: 0.492816  [ 6400/16040]\n",
            "loss: 0.232762  [ 8000/16040]\n",
            "loss: 0.427700  [ 9600/16040]\n",
            "loss: 0.514586  [11200/16040]\n",
            "loss: 0.247986  [12800/16040]\n",
            "loss: 0.276102  [14400/16040]\n",
            "loss: 0.220927  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 88.9%, Avg loss: 0.284054 \n",
            "\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 0.303656  [    0/16040]\n",
            "loss: 0.416832  [ 1600/16040]\n",
            "loss: 0.273404  [ 3200/16040]\n",
            "loss: 0.344199  [ 4800/16040]\n",
            "loss: 0.151719  [ 6400/16040]\n",
            "loss: 0.320577  [ 8000/16040]\n",
            "loss: 0.898816  [ 9600/16040]\n",
            "loss: 0.296414  [11200/16040]\n",
            "loss: 0.248163  [12800/16040]\n",
            "loss: 0.447761  [14400/16040]\n",
            "loss: 0.550496  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.1%, Avg loss: 0.234436 \n",
            "\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 0.636283  [    0/16040]\n",
            "loss: 0.439406  [ 1600/16040]\n",
            "loss: 0.441561  [ 3200/16040]\n",
            "loss: 0.228252  [ 4800/16040]\n",
            "loss: 0.390906  [ 6400/16040]\n",
            "loss: 0.475130  [ 8000/16040]\n",
            "loss: 0.474929  [ 9600/16040]\n",
            "loss: 0.375121  [11200/16040]\n",
            "loss: 0.575673  [12800/16040]\n",
            "loss: 0.939630  [14400/16040]\n",
            "loss: 0.482311  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.8%, Avg loss: 0.194370 \n",
            "\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 0.179935  [    0/16040]\n",
            "loss: 0.690450  [ 1600/16040]\n",
            "loss: 0.179694  [ 3200/16040]\n",
            "loss: 0.707417  [ 4800/16040]\n",
            "loss: 0.128596  [ 6400/16040]\n",
            "loss: 0.219992  [ 8000/16040]\n",
            "loss: 0.384913  [ 9600/16040]\n",
            "loss: 0.194510  [11200/16040]\n",
            "loss: 0.306558  [12800/16040]\n",
            "loss: 0.344437  [14400/16040]\n",
            "loss: 0.272799  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 92.5%, Avg loss: 0.210608 \n",
            "\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 0.592293  [    0/16040]\n",
            "loss: 0.188706  [ 1600/16040]\n",
            "loss: 0.247554  [ 3200/16040]\n",
            "loss: 0.404200  [ 4800/16040]\n",
            "loss: 0.156749  [ 6400/16040]\n",
            "loss: 0.599418  [ 8000/16040]\n",
            "loss: 0.227402  [ 9600/16040]\n",
            "loss: 0.240924  [11200/16040]\n",
            "loss: 0.220186  [12800/16040]\n",
            "loss: 0.285124  [14400/16040]\n",
            "loss: 0.093813  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.5%, Avg loss: 0.175261 \n",
            "\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 0.181663  [    0/16040]\n",
            "loss: 0.085395  [ 1600/16040]\n",
            "loss: 0.124559  [ 3200/16040]\n",
            "loss: 0.114881  [ 4800/16040]\n",
            "loss: 0.650406  [ 6400/16040]\n",
            "loss: 0.280645  [ 8000/16040]\n",
            "loss: 0.147974  [ 9600/16040]\n",
            "loss: 0.363680  [11200/16040]\n",
            "loss: 0.315722  [12800/16040]\n",
            "loss: 0.367256  [14400/16040]\n",
            "loss: 0.210133  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 91.1%, Avg loss: 0.215527 \n",
            "\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 0.188176  [    0/16040]\n",
            "loss: 0.240564  [ 1600/16040]\n",
            "loss: 0.281550  [ 3200/16040]\n",
            "loss: 0.147820  [ 4800/16040]\n",
            "loss: 0.381112  [ 6400/16040]\n",
            "loss: 0.277293  [ 8000/16040]\n",
            "loss: 0.472916  [ 9600/16040]\n",
            "loss: 0.210056  [11200/16040]\n",
            "loss: 0.036845  [12800/16040]\n",
            "loss: 0.151394  [14400/16040]\n",
            "loss: 0.380898  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 92.4%, Avg loss: 0.210258 \n",
            "\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 0.509342  [    0/16040]\n",
            "loss: 0.404842  [ 1600/16040]\n",
            "loss: 0.391110  [ 3200/16040]\n",
            "loss: 0.504377  [ 4800/16040]\n",
            "loss: 0.354574  [ 6400/16040]\n",
            "loss: 0.411650  [ 8000/16040]\n",
            "loss: 0.133013  [ 9600/16040]\n",
            "loss: 0.062926  [11200/16040]\n",
            "loss: 0.093302  [12800/16040]\n",
            "loss: 0.131383  [14400/16040]\n",
            "loss: 0.445239  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 95.5%, Avg loss: 0.128616 \n",
            "\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 0.285651  [    0/16040]\n",
            "loss: 0.253249  [ 1600/16040]\n",
            "loss: 0.291498  [ 3200/16040]\n",
            "loss: 0.139984  [ 4800/16040]\n",
            "loss: 0.294700  [ 6400/16040]\n",
            "loss: 0.167395  [ 8000/16040]\n",
            "loss: 0.152116  [ 9600/16040]\n",
            "loss: 0.314880  [11200/16040]\n",
            "loss: 0.419010  [12800/16040]\n",
            "loss: 0.279500  [14400/16040]\n",
            "loss: 0.594127  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.528402 \n",
            "\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 1.659777  [    0/16040]\n",
            "loss: 0.712809  [ 1600/16040]\n",
            "loss: 0.478103  [ 3200/16040]\n",
            "loss: 0.219484  [ 4800/16040]\n",
            "loss: 0.073568  [ 6400/16040]\n",
            "loss: 0.086408  [ 8000/16040]\n",
            "loss: 0.205083  [ 9600/16040]\n",
            "loss: 0.417326  [11200/16040]\n",
            "loss: 0.358105  [12800/16040]\n",
            "loss: 0.158591  [14400/16040]\n",
            "loss: 0.325410  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 91.6%, Avg loss: 0.226344 \n",
            "\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 0.752851  [    0/16040]\n",
            "loss: 0.146495  [ 1600/16040]\n",
            "loss: 0.340881  [ 3200/16040]\n",
            "loss: 0.152067  [ 4800/16040]\n",
            "loss: 0.159051  [ 6400/16040]\n",
            "loss: 0.075918  [ 8000/16040]\n",
            "loss: 0.048629  [ 9600/16040]\n",
            "loss: 0.114399  [11200/16040]\n",
            "loss: 0.183880  [12800/16040]\n",
            "loss: 0.157600  [14400/16040]\n",
            "loss: 0.185718  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.089890 \n",
            "\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 0.038874  [    0/16040]\n",
            "loss: 0.253824  [ 1600/16040]\n",
            "loss: 0.160982  [ 3200/16040]\n",
            "loss: 0.396852  [ 4800/16040]\n",
            "loss: 0.123819  [ 6400/16040]\n",
            "loss: 0.456577  [ 8000/16040]\n",
            "loss: 0.463805  [ 9600/16040]\n",
            "loss: 0.187350  [11200/16040]\n",
            "loss: 0.128212  [12800/16040]\n",
            "loss: 0.261951  [14400/16040]\n",
            "loss: 0.171667  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.081165 \n",
            "\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 0.101216  [    0/16040]\n",
            "loss: 0.390858  [ 1600/16040]\n",
            "loss: 0.251183  [ 3200/16040]\n",
            "loss: 0.229167  [ 4800/16040]\n",
            "loss: 0.310427  [ 6400/16040]\n",
            "loss: 0.144529  [ 8000/16040]\n",
            "loss: 0.104790  [ 9600/16040]\n",
            "loss: 0.278506  [11200/16040]\n",
            "loss: 0.316318  [12800/16040]\n",
            "loss: 0.249781  [14400/16040]\n",
            "loss: 0.206732  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 92.9%, Avg loss: 0.195034 \n",
            "\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 0.169181  [    0/16040]\n",
            "loss: 0.317921  [ 1600/16040]\n",
            "loss: 0.367066  [ 3200/16040]\n",
            "loss: 0.115871  [ 4800/16040]\n",
            "loss: 0.298204  [ 6400/16040]\n",
            "loss: 0.143602  [ 8000/16040]\n",
            "loss: 0.146185  [ 9600/16040]\n",
            "loss: 1.020264  [11200/16040]\n",
            "loss: 0.200947  [12800/16040]\n",
            "loss: 0.134454  [14400/16040]\n",
            "loss: 0.260832  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.1%, Avg loss: 0.089868 \n",
            "\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 0.235304  [    0/16040]\n",
            "loss: 0.191125  [ 1600/16040]\n",
            "loss: 0.254729  [ 3200/16040]\n",
            "loss: 0.043605  [ 4800/16040]\n",
            "loss: 0.134911  [ 6400/16040]\n",
            "loss: 0.582882  [ 8000/16040]\n",
            "loss: 0.126401  [ 9600/16040]\n",
            "loss: 0.193146  [11200/16040]\n",
            "loss: 0.136647  [12800/16040]\n",
            "loss: 0.082196  [14400/16040]\n",
            "loss: 0.127064  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.8%, Avg loss: 0.125675 \n",
            "\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 0.265402  [    0/16040]\n",
            "loss: 0.221300  [ 1600/16040]\n",
            "loss: 0.149492  [ 3200/16040]\n",
            "loss: 0.065646  [ 4800/16040]\n",
            "loss: 0.366690  [ 6400/16040]\n",
            "loss: 0.105841  [ 8000/16040]\n",
            "loss: 0.349582  [ 9600/16040]\n",
            "loss: 0.161930  [11200/16040]\n",
            "loss: 0.102320  [12800/16040]\n",
            "loss: 0.083863  [14400/16040]\n",
            "loss: 0.304232  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 96.2%, Avg loss: 0.096107 \n",
            "\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 0.235503  [    0/16040]\n",
            "loss: 0.107210  [ 1600/16040]\n",
            "loss: 0.192080  [ 3200/16040]\n",
            "loss: 0.121508  [ 4800/16040]\n",
            "loss: 0.481069  [ 6400/16040]\n",
            "loss: 0.120805  [ 8000/16040]\n",
            "loss: 0.328224  [ 9600/16040]\n",
            "loss: 0.293572  [11200/16040]\n",
            "loss: 0.008804  [12800/16040]\n",
            "loss: 0.229072  [14400/16040]\n",
            "loss: 0.166485  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.057807 \n",
            "\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 0.221764  [    0/16040]\n",
            "loss: 0.091014  [ 1600/16040]\n",
            "loss: 0.241535  [ 3200/16040]\n",
            "loss: 0.153765  [ 4800/16040]\n",
            "loss: 0.207039  [ 6400/16040]\n",
            "loss: 0.232972  [ 8000/16040]\n",
            "loss: 0.043389  [ 9600/16040]\n",
            "loss: 0.242278  [11200/16040]\n",
            "loss: 0.269998  [12800/16040]\n",
            "loss: 0.150972  [14400/16040]\n",
            "loss: 0.224395  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.3%, Avg loss: 0.084678 \n",
            "\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 0.186207  [    0/16040]\n",
            "loss: 0.266185  [ 1600/16040]\n",
            "loss: 0.029477  [ 3200/16040]\n",
            "loss: 0.266502  [ 4800/16040]\n",
            "loss: 0.109728  [ 6400/16040]\n",
            "loss: 0.359381  [ 8000/16040]\n",
            "loss: 0.017435  [ 9600/16040]\n",
            "loss: 0.454943  [11200/16040]\n",
            "loss: 0.037460  [12800/16040]\n",
            "loss: 0.101799  [14400/16040]\n",
            "loss: 0.107934  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.9%, Avg loss: 0.042575 \n",
            "\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 0.196778  [    0/16040]\n",
            "loss: 0.124120  [ 1600/16040]\n",
            "loss: 0.031471  [ 3200/16040]\n",
            "loss: 0.304435  [ 4800/16040]\n",
            "loss: 0.113194  [ 6400/16040]\n",
            "loss: 0.034545  [ 8000/16040]\n",
            "loss: 0.191146  [ 9600/16040]\n",
            "loss: 0.126971  [11200/16040]\n",
            "loss: 0.050933  [12800/16040]\n",
            "loss: 0.080359  [14400/16040]\n",
            "loss: 0.254483  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.0%, Avg loss: 0.081503 \n",
            "\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 0.134370  [    0/16040]\n",
            "loss: 0.539760  [ 1600/16040]\n",
            "loss: 0.051339  [ 3200/16040]\n",
            "loss: 0.187275  [ 4800/16040]\n",
            "loss: 0.261145  [ 6400/16040]\n",
            "loss: 0.381539  [ 8000/16040]\n",
            "loss: 0.185646  [ 9600/16040]\n",
            "loss: 0.075182  [11200/16040]\n",
            "loss: 0.356936  [12800/16040]\n",
            "loss: 0.048014  [14400/16040]\n",
            "loss: 0.095408  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.062094 \n",
            "\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 0.062749  [    0/16040]\n",
            "loss: 0.031892  [ 1600/16040]\n",
            "loss: 0.111143  [ 3200/16040]\n",
            "loss: 0.026920  [ 4800/16040]\n",
            "loss: 0.041574  [ 6400/16040]\n",
            "loss: 0.049407  [ 8000/16040]\n",
            "loss: 0.079911  [ 9600/16040]\n",
            "loss: 0.163725  [11200/16040]\n",
            "loss: 0.070129  [12800/16040]\n",
            "loss: 0.038260  [14400/16040]\n",
            "loss: 0.065986  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.044813 \n",
            "\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 0.032087  [    0/16040]\n",
            "loss: 0.030339  [ 1600/16040]\n",
            "loss: 0.256342  [ 3200/16040]\n",
            "loss: 0.068695  [ 4800/16040]\n",
            "loss: 0.006287  [ 6400/16040]\n",
            "loss: 0.180188  [ 8000/16040]\n",
            "loss: 0.129545  [ 9600/16040]\n",
            "loss: 0.021580  [11200/16040]\n",
            "loss: 0.333691  [12800/16040]\n",
            "loss: 0.288880  [14400/16040]\n",
            "loss: 0.413242  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 98.5%, Avg loss: 0.061023 \n",
            "\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 0.034741  [    0/16040]\n",
            "loss: 0.038755  [ 1600/16040]\n",
            "loss: 0.072865  [ 3200/16040]\n",
            "loss: 0.164161  [ 4800/16040]\n",
            "loss: 0.101788  [ 6400/16040]\n",
            "loss: 0.121769  [ 8000/16040]\n",
            "loss: 0.425211  [ 9600/16040]\n",
            "loss: 0.306202  [11200/16040]\n",
            "loss: 0.007828  [12800/16040]\n",
            "loss: 0.069794  [14400/16040]\n",
            "loss: 0.057081  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 0.111420 \n",
            "\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 0.141663  [    0/16040]\n",
            "loss: 0.032262  [ 1600/16040]\n",
            "loss: 0.082802  [ 3200/16040]\n",
            "loss: 0.119920  [ 4800/16040]\n",
            "loss: 0.271278  [ 6400/16040]\n",
            "loss: 0.087609  [ 8000/16040]\n",
            "loss: 0.169606  [ 9600/16040]\n",
            "loss: 0.151120  [11200/16040]\n",
            "loss: 0.034457  [12800/16040]\n",
            "loss: 0.047109  [14400/16040]\n",
            "loss: 0.275335  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 93.3%, Avg loss: 0.192442 \n",
            "\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 0.246939  [    0/16040]\n",
            "loss: 0.047963  [ 1600/16040]\n",
            "loss: 0.063132  [ 3200/16040]\n",
            "loss: 0.083591  [ 4800/16040]\n",
            "loss: 0.018663  [ 6400/16040]\n",
            "loss: 0.039075  [ 8000/16040]\n",
            "loss: 0.203363  [ 9600/16040]\n",
            "loss: 0.710633  [11200/16040]\n",
            "loss: 0.063807  [12800/16040]\n",
            "loss: 0.071714  [14400/16040]\n",
            "loss: 0.047739  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.065621 \n",
            "\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 0.284053  [    0/16040]\n",
            "loss: 0.433480  [ 1600/16040]\n",
            "loss: 0.051101  [ 3200/16040]\n",
            "loss: 0.156791  [ 4800/16040]\n",
            "loss: 0.166560  [ 6400/16040]\n",
            "loss: 0.078160  [ 8000/16040]\n",
            "loss: 0.056846  [ 9600/16040]\n",
            "loss: 0.169184  [11200/16040]\n",
            "loss: 0.157429  [12800/16040]\n",
            "loss: 0.071592  [14400/16040]\n",
            "loss: 0.104673  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.067220 \n",
            "\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 0.181179  [    0/16040]\n",
            "loss: 0.040595  [ 1600/16040]\n",
            "loss: 0.087964  [ 3200/16040]\n",
            "loss: 0.238483  [ 4800/16040]\n",
            "loss: 0.108988  [ 6400/16040]\n",
            "loss: 0.103415  [ 8000/16040]\n",
            "loss: 0.311235  [ 9600/16040]\n",
            "loss: 0.028828  [11200/16040]\n",
            "loss: 0.001238  [12800/16040]\n",
            "loss: 0.007434  [14400/16040]\n",
            "loss: 0.091960  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 88.7%, Avg loss: 0.338380 \n",
            "\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 0.019939  [    0/16040]\n",
            "loss: 0.190542  [ 1600/16040]\n",
            "loss: 0.195287  [ 3200/16040]\n",
            "loss: 0.020285  [ 4800/16040]\n",
            "loss: 0.136282  [ 6400/16040]\n",
            "loss: 0.221542  [ 8000/16040]\n",
            "loss: 0.062886  [ 9600/16040]\n",
            "loss: 0.141814  [11200/16040]\n",
            "loss: 0.182634  [12800/16040]\n",
            "loss: 0.046923  [14400/16040]\n",
            "loss: 0.070946  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 94.9%, Avg loss: 0.148300 \n",
            "\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 0.298945  [    0/16040]\n",
            "loss: 0.164293  [ 1600/16040]\n",
            "loss: 0.090760  [ 3200/16040]\n",
            "loss: 0.086363  [ 4800/16040]\n",
            "loss: 0.003217  [ 6400/16040]\n",
            "loss: 0.026158  [ 8000/16040]\n",
            "loss: 0.060653  [ 9600/16040]\n",
            "loss: 0.395968  [11200/16040]\n",
            "loss: 0.135840  [12800/16040]\n",
            "loss: 0.014137  [14400/16040]\n",
            "loss: 0.204220  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.4%, Avg loss: 0.026108 \n",
            "\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 0.045340  [    0/16040]\n",
            "loss: 0.011753  [ 1600/16040]\n",
            "loss: 0.028557  [ 3200/16040]\n",
            "loss: 0.567905  [ 4800/16040]\n",
            "loss: 0.052495  [ 6400/16040]\n",
            "loss: 0.001683  [ 8000/16040]\n",
            "loss: 0.121302  [ 9600/16040]\n",
            "loss: 0.078474  [11200/16040]\n",
            "loss: 0.025735  [12800/16040]\n",
            "loss: 0.035747  [14400/16040]\n",
            "loss: 0.048506  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.012199 \n",
            "\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 0.016860  [    0/16040]\n",
            "loss: 0.101965  [ 1600/16040]\n",
            "loss: 0.020210  [ 3200/16040]\n",
            "loss: 0.140865  [ 4800/16040]\n",
            "loss: 0.287484  [ 6400/16040]\n",
            "loss: 0.003464  [ 8000/16040]\n",
            "loss: 0.208055  [ 9600/16040]\n",
            "loss: 0.251880  [11200/16040]\n",
            "loss: 0.059740  [12800/16040]\n",
            "loss: 0.193443  [14400/16040]\n",
            "loss: 0.128220  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.7%, Avg loss: 0.021971 \n",
            "\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 0.063917  [    0/16040]\n",
            "loss: 0.063171  [ 1600/16040]\n",
            "loss: 0.023100  [ 3200/16040]\n",
            "loss: 0.032986  [ 4800/16040]\n",
            "loss: 0.592789  [ 6400/16040]\n",
            "loss: 0.031944  [ 8000/16040]\n",
            "loss: 0.103605  [ 9600/16040]\n",
            "loss: 0.445421  [11200/16040]\n",
            "loss: 0.005919  [12800/16040]\n",
            "loss: 0.001932  [14400/16040]\n",
            "loss: 0.087808  [16000/16040]\n",
            "Test Error: \n",
            " Accuracy: 99.9%, Avg loss: 0.012248 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "    print('\\n Accuracy of SGD with dropout layer on test data is:\\n')\n",
        "    test(Test_dataloader, model3, loss_fn)\n",
        "    print('\\n Accuracy of ADAM with dropout layer on test data is:\\n')\n",
        "    test(Test_dataloader, model4, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGnodzm6Y9WQ",
        "outputId": "ecc9f332-496a-472f-d281-9c62874c7ae6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Accuracy of SGD with dropout layer on test data is:\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 1.056544 \n",
            "\n",
            "\n",
            " Accuracy of ADAM with dropout layer on test data is:\n",
            "\n",
            "Test Error: \n",
            " Accuracy: 72.5%, Avg loss: 2.270158 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical results show that although the learning rate is much slower, models with the Adam optimizer learn very quickly in comparison with the SGD optimizer. On the other hand, the accuracy of the models with Adam optimizer on the test data is less than SGD.\n",
        "\n",
        "Although models without dropout layers have 100% accuracy on the validation set, Numerical results show that models with dropout layers have higher accuracy on the test data, which shows the effect of generalization.\n",
        "\n",
        "In conclusion, the model with SGD optimizer + dropout layers has the best performance (78.6%) on the test data.:\n"
      ],
      "metadata": {
        "id": "oPWOAlJ9tB9z"
      }
    }
  ]
}